{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt at finetuning gpt2 on pushkin poems.\n",
    "doesn't work yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# display all outputs from Jupyter notebook cells, not just last.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch pretrained model as base to finetune\n",
    "base_checkpoint = 'ai-forever/rugpt3small_based_on_gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"abobster/pushkin_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    # return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=2048, return_tensors=\"pt\").to(device)\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").to(device)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune parameters\n",
    "\n",
    "current_model = './fine_tuned_pushkin_gpt2'\n",
    "previous_model = 'ai-forever/rugpt3small_based_on_gpt2'\n",
    "output_dir='./results'\n",
    "num_train_epochs = 2\n",
    "per_device_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(previous_model).to(device)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)\n",
    "\n",
    "tr_args = TrainingArguments(\n",
    "    output_dir=output_dir, \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    "    use_cpu = False,\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tr_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    data_collator=data_collator)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/rugpt3small_based_on_gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained(current_model)\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=100, num_return_sequences=1):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "prompt = \"У лукоморья дуб\"\n",
    "generated_texts = generate_text(prompt, model, tokenizer)\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f'Generated Text {i + 1}:')\n",
    "    print(text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
