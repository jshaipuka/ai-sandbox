{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Homework for AI-For-Beginners course: 04 - Own Framework](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all outputs from Jupyter notebook cells, not just last.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# pick the seed for reproducability - change it to explore the effects of random variations\n",
    "np.random.seed(1)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mnist\")\n",
    "dataset_train, dataset_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(dataset_train['image']).reshape(-1, 784) # train_x.shape = (60000, 28, 28) -> (60000, 784)\n",
    "train_labels = dataset_train['label'] \n",
    "\n",
    "test_x = np.array(dataset_test['image']).reshape(-1, 784) # test_x.shape = (10000, 28, 28) -> (10000, 784)\n",
    "test_labels = dataset_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # nin - number of input features, nout - number of output clasess\n",
    "    def __init__(self,nin,nout):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin)) #weights\n",
    "        self.b = np.zeros((1,nout)) # bias  vector\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x=x\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "    \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n",
    "\n",
    "# ReLU (Rectified Linear Unit) is an activation function used to introduce non-linearity into the network.\n",
    "# It operates element-wise on the input matrix and replaces all negative pixel values in the feature map by zero. The function is f(x) = max(0, x).\n",
    "# It's often used in the hidden layers of a neural network.\n",
    "class ReLU:\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def backward(self, dp):\n",
    "        dz = np.where(self.z > 0, 1.0, 0.0)\n",
    "        return dp * dz\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        self.z = z # z.shape = (batch_size, num_classes). z.shape = (60000, 10)\n",
    "        zmax = z.max(axis=1,keepdims=True) #zmax.shape = (60000, 1, 28)\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        return expz / Z\n",
    "    def backward(self,dp):\n",
    "        p = self.forward(self.z)\n",
    "        pdp = p * dp\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "class CrossEntropyLoss:\n",
    "    def forward(self,p,y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y + 1e-9)  # Add a small constant to prevent log(0)\n",
    "        return -log_prob.mean()\n",
    "    def backward(self,loss):\n",
    "        dlog_softmax = np.zeros_like(self.p)\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\n",
    "        return dlog_softmax / (self.p + 1e-9)  # Add a small constant to prevent division by zero\n",
    "\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_acc(x,y,loss=CrossEntropyLoss()):\n",
    "    p = net.forward(x)\n",
    "    l = loss.forward(p,y)\n",
    "    pred = np.argmax(p,axis=1)\n",
    "    acc = (pred==y).mean()\n",
    "    return l,acc\n",
    "\n",
    "def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):\n",
    "    for i in range(0,len(train_x),batch_size):\n",
    "        xb = train_x[i:i+batch_size]\n",
    "        yb = train_labels[i:i+batch_size]\n",
    "\n",
    "        p = net.forward(xb) \n",
    "        l = loss.forward(p,yb)\n",
    "        dp = loss.backward(l)\n",
    "        dx = net.backward(dp)\n",
    "        net.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=19.06245656631888, accuracy=0.0655: \n",
      "Final loss=4.352638784997269, accuracy=0.7744333333333333: \n",
      "Test loss=4.162335114860613, accuracy=0.7846: \n"
     ]
    }
   ],
   "source": [
    "# 1 layer network\n",
    "net = Net()\n",
    "net.add(Linear(28*28, 10)) # 28*28 = 784 is the number of input features (pixels. a.k.a. a digit is represented as png file by those dimensions), 10 is the number of output features (classes. a.k.a. expected digits 0-9)\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "\n",
    "train_epoch(net,train_x,train_labels, batch_size=320, lr=0.001)\n",
    "        \n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(test_x,test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=17.070339247873864, accuracy=0.12121666666666667: \n",
      "Final loss=3.7820837971444985, accuracy=0.78795: \n",
      "Test loss=3.7585837164578204, accuracy=0.7902: \n"
     ]
    }
   ],
   "source": [
    "# 2 layer network\n",
    "net = Net()\n",
    "net.add(Linear(28*28, 100)) # First layer with 100 neurons\n",
    "net.add(ReLU())  # Non-linear activation function\n",
    "net.add(Linear(100, 10)) # Second layer with 10 neurons (one for each class)\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "\n",
    "train_epoch(net,train_x,train_labels, batch_size=320, lr=0.001)\n",
    "        \n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(test_x,test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-for-beginners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
