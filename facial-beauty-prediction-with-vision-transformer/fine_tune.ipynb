{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:54:07.342977Z",
     "start_time": "2025-07-12T16:54:05.146090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "6dd317204a8f0985",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/facial-beauty-prediction-with-vision-transformer/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:54:09.437307Z",
     "start_time": "2025-07-12T16:54:09.407433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "4b99f81b223df85a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T16:54:13.565003Z",
     "start_time": "2025-07-12T16:54:12.439551Z"
    }
   },
   "source": [
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n",
    "model = model.to(device)\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "print(config)\n",
    "transform = create_transform(**config)\n",
    "print(transform)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'crop_pct': 0.9, 'crop_mode': 'center'}\n",
      "Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:54:46.605795Z",
     "start_time": "2025-07-12T16:54:45.389785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "img = Image.open(filename).convert('RGB')\n",
    "tensor = transform(img).unsqueeze(0)\n",
    "print(tensor.shape)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "model(tensor)"
   ],
   "id": "3690fac725714555",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2534]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:54:58.629102Z",
     "start_time": "2025-07-12T16:54:58.118501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        outputs_dir = os.path.join(os.getcwd(), \"outputs\")\n",
    "        dataset_file = os.path.join(outputs_dir, 'dataset', 'dataset.pt')\n",
    "        x, y, sigma = torch.load(dataset_file)\n",
    "        self.x = x\n",
    "        self.y = y.unsqueeze(dim=-1)\n",
    "        self.sigma = sigma\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: data transformers\n",
    "        image = self.x[idx]\n",
    "        return image if self.transform is None else self.transform(image), self.y[idx]\n",
    "\n",
    "\n",
    "dataset = CustomImageDataset(transform=transform)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])"
   ],
   "id": "3e95ce39b65bfb86",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:55:00.651570Z",
     "start_time": "2025-07-12T16:55:00.648811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "5e0357dde004de41",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:55:01.660366Z",
     "start_time": "2025-07-12T16:55:01.627548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 6\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "outputs_dir = os.path.join(os.getcwd(), \"outputs\")\n",
    "cv2.imwrite(os.path.join(outputs_dir, f\"output_{index}.jpg\"), train_features[index].numpy().transpose((1, 2, 0)))"
   ],
   "id": "797b868d75b49533",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@56.054] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:55:07.774433Z",
     "start_time": "2025-07-12T16:55:06.600920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features = train_features.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "model(train_features)\n"
   ],
   "id": "78cfdb516058c23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9417],\n",
       "        [-2.9615],\n",
       "        [-3.4184],\n",
       "        [-2.7525],\n",
       "        [-3.4995],\n",
       "        [-3.3067],\n",
       "        [-3.2270],\n",
       "        [-2.8109],\n",
       "        [-3.4074],\n",
       "        [-3.0861],\n",
       "        [-3.2146],\n",
       "        [-3.0557],\n",
       "        [-2.9839],\n",
       "        [-2.7279],\n",
       "        [-3.1739],\n",
       "        [-2.4951],\n",
       "        [-2.4629],\n",
       "        [-3.0998],\n",
       "        [-2.9713],\n",
       "        [-2.5996],\n",
       "        [-2.5224],\n",
       "        [-2.8620],\n",
       "        [-3.0789],\n",
       "        [-3.2175],\n",
       "        [-3.0509],\n",
       "        [-3.2535],\n",
       "        [-2.8265],\n",
       "        [-2.5304],\n",
       "        [-2.5634],\n",
       "        [-3.0118],\n",
       "        [-3.1917],\n",
       "        [-3.3138],\n",
       "        [-3.2846],\n",
       "        [-2.8126],\n",
       "        [-3.1080],\n",
       "        [-3.2125],\n",
       "        [-2.2847],\n",
       "        [-3.0296],\n",
       "        [-3.2456],\n",
       "        [-2.9109],\n",
       "        [-2.4987],\n",
       "        [-2.8149],\n",
       "        [-3.1199],\n",
       "        [-3.0501],\n",
       "        [-2.7353],\n",
       "        [-3.4728],\n",
       "        [-2.7140],\n",
       "        [-2.5155],\n",
       "        [-2.7812],\n",
       "        [-3.2834],\n",
       "        [-3.1311],\n",
       "        [-2.9152],\n",
       "        [-2.5764],\n",
       "        [-2.6191],\n",
       "        [-2.9708],\n",
       "        [-2.8948],\n",
       "        [-2.9006],\n",
       "        [-2.7349],\n",
       "        [-3.0779],\n",
       "        [-2.5972],\n",
       "        [-2.6190],\n",
       "        [-3.2579],\n",
       "        [-2.9045],\n",
       "        [-2.6469]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:55:14.858966Z",
     "start_time": "2025-07-12T16:55:14.854397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = MSELoss()"
   ],
   "id": "5ff210832a0dde20",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T16:55:19.385405Z",
     "start_time": "2025-07-12T16:55:16.366430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "for x, y in train_dataloader:\n",
    "    x, y = x.to(device), torch.reshape(y, shape=(64, 1)).to(device)\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Loss is {loss}')"
   ],
   "id": "b778284f444c0689",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m loss = criterion(outputs, y)\n\u001B[32m      6\u001B[39m optimizer.zero_grad()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m loss.backward()\n\u001B[32m      8\u001B[39m optimizer.step()\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mLoss is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/facial-beauty-prediction-with-vision-transformer/lib/python3.12/site-packages/torch/_tensor.py:624\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    614\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    615\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    616\u001B[39m         Tensor.backward,\n\u001B[32m    617\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    622\u001B[39m         inputs=inputs,\n\u001B[32m    623\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m624\u001B[39m torch.autograd.backward(\n\u001B[32m    625\u001B[39m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001B[32m    626\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/facial-beauty-prediction-with-vision-transformer/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m _engine_run_backward(\n\u001B[32m    348\u001B[39m     tensors,\n\u001B[32m    349\u001B[39m     grad_tensors_,\n\u001B[32m    350\u001B[39m     retain_graph,\n\u001B[32m    351\u001B[39m     create_graph,\n\u001B[32m    352\u001B[39m     inputs,\n\u001B[32m    353\u001B[39m     allow_unreachable=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    354\u001B[39m     accumulate_grad=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    355\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/facial-beauty-prediction-with-vision-transformer/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    823\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable._execution_engine.run_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m         t_outputs, *args, **kwargs\n\u001B[32m    827\u001B[39m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    829\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22e37fbde2465140"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
