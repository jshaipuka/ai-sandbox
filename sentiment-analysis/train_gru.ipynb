{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T14:53:14.666638Z",
     "start_time": "2025-07-01T14:53:14.662821Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:53:15.039775Z",
     "start_time": "2025-07-01T14:53:15.036726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:55:16.078875Z",
     "start_time": "2025-07-01T14:54:10.042041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in messages:\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc if token.pos_ in {'ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB'}]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = torch.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector\n",
    "\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "validation_dataset: Dataset = dataset['validation'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "test_dataset: Dataset = dataset['test']\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset['text'])\n",
    "validation_tokens, validation_tokenized_messages = tokenize_messages(validation_dataset['text'])\n",
    "\n",
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "524dccea61b74a90b64e1fcce19c9e19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "115a246dc2bf403496780f1a802a45e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "742552819ab94ad5aa2b670f7785904e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4279 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9dcdbf5a2f94c03b733602c2e1ef63b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '%', '&', \"'\", \"'back\", \"'calendar\", \"'gummed\", \"'not\", \"'peter\", \"'s\", '(:', '(=', '*', '****', '***kix', '*shuts', '*whispers', '+', '+1/2']\n",
      "25906\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:55:19.655018Z",
     "start_time": "2025-07-01T14:55:19.644741Z"
    }
   },
   "cell_type": "code",
   "source": "token_to_index = {u: i for i, u in enumerate(vocabulary)}",
   "id": "1afd5c566494b4ee",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:56:33.852161Z",
     "start_time": "2025-07-01T14:56:33.609548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "validation_messages = [encode_x(token_to_index, tokens) for tokens in validation_tokenized_messages]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(validation_messages))\n",
    "print(len(validation_labels))"
   ],
   "id": "76c44cb0c0b867a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25913\n",
      "25913\n",
      "4279\n",
      "4279\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:58:33.039656Z",
     "start_time": "2025-07-01T14:58:33.036784Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5d91d7d27be6a117",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiderwoman',\n",
       " 'she`s',\n",
       " 'amazing',\n",
       " 'mum',\n",
       " 'gr8',\n",
       " 'blogger',\n",
       " 'gr8',\n",
       " 'mentor',\n",
       " 'top',\n",
       " 'climbs',\n",
       " 'walls',\n",
       " 'http://digg.com/d1qeua']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:10:06.633133Z",
     "start_time": "2025-07-01T15:10:06.628065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size, padding_value):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True, padding_value=padding_value).int(), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "3f23bbdb9a1ce2ad",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:10:07.534812Z",
     "start_time": "2025-07-01T15:10:07.530138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        prediction_with_activation = self.activation(prediction[:, -1, :])\n",
    "        return self.linear(prediction_with_activation), hidden"
   ],
   "id": "74d00197ac7e8079",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:11:04.591426Z",
     "start_time": "2025-07-01T15:10:29.038650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: try different batch sizes\n",
    "batch_size = 128\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "embedding_dim = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 1000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_messages, train_labels, batch_size, len(vocabulary))\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == number_of_epoches - 1:\n",
    "        print(f'Epoch {epoch}, train loss {loss.item()}')"
   ],
   "id": "272aafaf0962bf85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 1.0987062454223633\n",
      "Epoch 10, train loss 1.0459595918655396\n",
      "Epoch 20, train loss 1.1357064247131348\n",
      "Epoch 30, train loss 1.1586110591888428\n",
      "Epoch 40, train loss 0.9328310489654541\n",
      "Epoch 50, train loss 0.8262327909469604\n",
      "Epoch 60, train loss 1.0850646495819092\n",
      "Epoch 70, train loss 0.9762576818466187\n",
      "Epoch 80, train loss 0.8735822439193726\n",
      "Epoch 90, train loss 0.9356590509414673\n",
      "Epoch 100, train loss 0.9574069380760193\n",
      "Epoch 110, train loss 0.5967537760734558\n",
      "Epoch 120, train loss 0.6612371206283569\n",
      "Epoch 130, train loss 0.6998139023780823\n",
      "Epoch 140, train loss 0.9851012825965881\n",
      "Epoch 150, train loss 0.8151595592498779\n",
      "Epoch 160, train loss 0.595989465713501\n",
      "Epoch 170, train loss 0.7471057772636414\n",
      "Epoch 180, train loss 0.7372685074806213\n",
      "Epoch 190, train loss 0.5491386651992798\n",
      "Epoch 200, train loss 0.8636525869369507\n",
      "Epoch 210, train loss 0.9624097347259521\n",
      "Epoch 220, train loss 0.6215113401412964\n",
      "Epoch 230, train loss 0.6260864734649658\n",
      "Epoch 240, train loss 0.9285305738449097\n",
      "Epoch 250, train loss 0.5942163467407227\n",
      "Epoch 260, train loss 0.4171653985977173\n",
      "Epoch 270, train loss 0.7763100862503052\n",
      "Epoch 280, train loss 0.6502547264099121\n",
      "Epoch 290, train loss 0.5107203722000122\n",
      "Epoch 300, train loss 0.7522900104522705\n",
      "Epoch 310, train loss 0.43048095703125\n",
      "Epoch 320, train loss 0.5911162495613098\n",
      "Epoch 330, train loss 0.2869296669960022\n",
      "Epoch 340, train loss 0.7847656011581421\n",
      "Epoch 350, train loss 0.3796497583389282\n",
      "Epoch 360, train loss 0.538210391998291\n",
      "Epoch 370, train loss 0.4854983389377594\n",
      "Epoch 380, train loss 0.5041297674179077\n",
      "Epoch 390, train loss 0.74163419008255\n",
      "Epoch 400, train loss 0.7377148866653442\n",
      "Epoch 410, train loss 0.5083670616149902\n",
      "Epoch 420, train loss 0.7140213251113892\n",
      "Epoch 430, train loss 0.4729500412940979\n",
      "Epoch 440, train loss 0.7679047584533691\n",
      "Epoch 450, train loss 0.5132448077201843\n",
      "Epoch 460, train loss 0.47953301668167114\n",
      "Epoch 470, train loss 0.4822691082954407\n",
      "Epoch 480, train loss 0.6353173851966858\n",
      "Epoch 490, train loss 0.8482348322868347\n",
      "Epoch 500, train loss 0.6356948614120483\n",
      "Epoch 510, train loss 0.44838181138038635\n",
      "Epoch 520, train loss 0.48551368713378906\n",
      "Epoch 530, train loss 0.2670149803161621\n",
      "Epoch 540, train loss 0.5063289999961853\n",
      "Epoch 550, train loss 0.6782875061035156\n",
      "Epoch 560, train loss 0.651394248008728\n",
      "Epoch 570, train loss 1.109603762626648\n",
      "Epoch 580, train loss 0.43111681938171387\n",
      "Epoch 590, train loss 0.3010188937187195\n",
      "Epoch 600, train loss 0.2954716980457306\n",
      "Epoch 610, train loss 0.2623632550239563\n",
      "Epoch 620, train loss 0.37998783588409424\n",
      "Epoch 630, train loss 0.3068733811378479\n",
      "Epoch 640, train loss 0.5718245506286621\n",
      "Epoch 650, train loss 0.7532050609588623\n",
      "Epoch 660, train loss 0.5488333702087402\n",
      "Epoch 670, train loss 0.601447343826294\n",
      "Epoch 680, train loss 0.6707395911216736\n",
      "Epoch 690, train loss 0.6771944761276245\n",
      "Epoch 700, train loss 0.4771806299686432\n",
      "Epoch 710, train loss 0.8102972507476807\n",
      "Epoch 720, train loss 0.5490814447402954\n",
      "Epoch 730, train loss 0.6256067752838135\n",
      "Epoch 740, train loss 0.6068269610404968\n",
      "Epoch 750, train loss 0.8861868381500244\n",
      "Epoch 760, train loss 0.5981433391571045\n",
      "Epoch 770, train loss 0.8094314932823181\n",
      "Epoch 780, train loss 0.5269455909729004\n",
      "Epoch 790, train loss 0.5529322624206543\n",
      "Epoch 800, train loss 0.9549763798713684\n",
      "Epoch 810, train loss 1.0801429748535156\n",
      "Epoch 820, train loss 0.7820546627044678\n",
      "Epoch 830, train loss 0.5803594589233398\n",
      "Epoch 840, train loss 0.49970942735671997\n",
      "Epoch 850, train loss 0.5719782710075378\n",
      "Epoch 860, train loss 0.4446709454059601\n",
      "Epoch 870, train loss 0.9185149073600769\n",
      "Epoch 880, train loss 0.44728076457977295\n",
      "Epoch 890, train loss 0.8334305286407471\n",
      "Epoch 900, train loss 0.5199376344680786\n",
      "Epoch 910, train loss 1.1885664463043213\n",
      "Epoch 920, train loss 0.7723212838172913\n",
      "Epoch 930, train loss 0.8811906576156616\n",
      "Epoch 940, train loss 0.72508704662323\n",
      "Epoch 950, train loss 0.6883495450019836\n",
      "Epoch 960, train loss 0.7796002626419067\n",
      "Epoch 970, train loss 0.6939356923103333\n",
      "Epoch 980, train loss 0.8508874177932739\n",
      "Epoch 990, train loss 0.5197867751121521\n",
      "Epoch 999, train loss 0.765983521938324\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:15:52.278920Z",
     "start_time": "2025-07-01T15:15:52.215792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "index = 1004\n",
    "message = validation_dataset['text'][index]\n",
    "label = validation_dataset['label'][index]\n",
    "print(message)\n",
    "print(label)\n",
    "encoded_message = encode_x(token_to_index, validation_tokenized_messages[index])\n",
    "encoded_label = encode_y(label)\n",
    "print(encoded_message)\n",
    "print(encoded_label)\n",
    "x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "print(distribution)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[label])\n",
    "print(labels[torch.argmax(distribution)])"
   ],
   "id": "e2727732916a2a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinks Aaron is pretty darn awesome\n",
      "2\n",
      "tensor([22488,   551, 17816,  5625,  1902])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[0.0185, 0.0670, 0.9145]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:23:20.336851Z",
     "start_time": "2025-07-01T15:23:20.282087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I hated you\" # \"I never hated you\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch, _ = create_batch([encoded_message], [torch.tensor([0, 0, 0])], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "print(distribution)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[torch.argmax(distribution)])"
   ],
   "id": "8ec28040b1a09fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8598, 0.1303, 0.0099]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "negative\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bba20ab64630ba51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
