{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:14:06.071887Z",
     "start_time": "2025-07-15T07:14:06.067588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import comgra\n",
    "from comgra.objects import DecisionMakerForRecordingsFrequencyPerType\n",
    "from comgra.recorder import ComgraRecorder"
   ],
   "id": "a9dea8914236e91",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:14:08.989817Z",
     "start_time": "2025-07-15T07:14:08.986379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "cc76c8b605ea8925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:14:09.618814Z",
     "start_time": "2025-07-15T07:14:09.613897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in tqdm(messages):\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "# Returns a one-hot encoding of a label, e.g., (0, 1, 0).\n",
    "def encode_y(label):\n",
    "    y = torch.zeros(3)\n",
    "    y[label] = 1\n",
    "    return y"
   ],
   "id": "163dd410cddbc3f7",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:16:02.578316Z",
     "start_time": "2025-07-15T07:14:12.739737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "max_length = float('inf')\n",
    "dataset_huggingface = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset_huggingface: Dataset = dataset_huggingface['train'].filter(lambda it: len(it['text']) <= max_length).map(process).sort('length')\n",
    "val_dataset_huggingface: Dataset = dataset_huggingface['validation'].filter(lambda it: len(it['text']) <= max_length).map(process).sort(\n",
    "    'length')\n",
    "test_dataset_huggingface: Dataset = dataset_huggingface['test'].filter(lambda it: it['text'] is not None and len(it['text']) <= max_length).map(\n",
    "    process).sort('length')\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset_huggingface['text'])\n",
    "validation_tokens, val_tokenized_messages = tokenize_messages(val_dataset_huggingface['text'])\n",
    "test_tokens, test_tokenized_messages = tokenize_messages(test_dataset_huggingface['text'])"
   ],
   "id": "6f85cdf981c20f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed unexpectedly!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df39e83b735449758c42b104146613ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c7848f353b4ea2bd107287a0c4f2e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d99f9852c2b4588aa451431635aeb45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31232/31232 [01:18<00:00, 396.43it/s]\n",
      "100%|██████████| 5205/5205 [00:13<00:00, 395.60it/s]\n",
      "100%|██████████| 5205/5205 [00:12<00:00, 404.46it/s]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:16:18.919522Z",
     "start_time": "2025-07-15T07:16:18.876156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens + test_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))\n",
    "\n",
    "token_to_index = {u: i for i, u in enumerate(vocabulary)}"
   ],
   "id": "83a48a58a92b318c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t ', '\\n', '\\n\\n', ' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ', '             ', '              ', '               ', '                ', '                                           ', '                                                                                              ', '!', '\"', '\"-']\n",
      "36633\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:16:21.102025Z",
     "start_time": "2025-07-15T07:16:20.512679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset_huggingface['label']]\n",
    "val_messages = [encode_x(token_to_index, tokens) for tokens in val_tokenized_messages]\n",
    "val_labels = [encode_y(label) for label in val_dataset_huggingface['label']]\n",
    "test_messages = [encode_x(token_to_index, tokens) for tokens in test_tokenized_messages]\n",
    "test_labels = [encode_y(label) for label in test_dataset_huggingface['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(val_messages))\n",
    "print(len(val_labels))\n",
    "print(len(test_messages))\n",
    "print(len(test_labels))"
   ],
   "id": "890b7a1117c53c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31232\n",
      "31232\n",
      "5205\n",
      "5205\n",
      "5205\n",
      "5205\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:16:23.176206Z",
     "start_time": "2025-07-15T07:16:23.094088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 20000\n",
    "print(train_dataset_huggingface['text'][index], train_tokenized_messages[index])"
   ],
   "id": "5377b20cad482166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least he`s in breakthrough performance tho. I just wanted him nominated in his own category ['at', 'least', 'he`s', 'in', 'breakthrough', 'performance', 'tho', '.', 'i', 'just', 'wanted', 'him', 'nominated', 'in', 'his', 'own', 'category']\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:16:28.146655Z",
     "start_time": "2025-07-15T07:16:28.104787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_dataset = CustomDataset(train_messages, train_labels)\n",
    "val_dataset = CustomDataset(val_messages, val_labels)\n",
    "test_dataset = CustomDataset(test_messages, test_labels)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO: a way to pass padding_value without closure?\n",
    "    return [\n",
    "        nn.utils.rnn.pad_sequence([x[0] for x in batch], batch_first=True, padding_value=len(vocabulary)),\n",
    "        torch.stack([x[1] for x in batch])\n",
    "    ]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = False\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)"
   ],
   "id": "5ecf640629283f3",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:26:23.860372Z",
     "start_time": "2025-07-15T07:26:23.854795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "        self.log_softmax = nn.LogSoftmax(\n",
    "            dim=1)  # The negative log likelihood loss expects log-probabilities of each class.\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        linear_prediction = self.linear(prediction[:, -1])\n",
    "        return self.log_softmax(linear_prediction), hidden"
   ],
   "id": "60774ac30b3db966",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:26:24.350080Z",
     "start_time": "2025-07-15T07:26:24.328027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comgra.my_recorder = ComgraRecorder(\n",
    "    comgra_root_path=os.path.join(os.getcwd(), \"comgra\"),\n",
    "    group=\"name_of_experiment_group\",\n",
    "    trial_id=\"example_trial\",\n",
    "    decision_maker_for_recordings=DecisionMakerForRecordingsFrequencyPerType(min_training_steps_difference=5),\n",
    ")\n",
    "comgra.my_recorder.add_note(\"This is an optional log message that will show up in the 'Notes' tab.\")\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "embedding_dim, hidden_size, num_layers = 4, 128, 1\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "# See https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "comgra.my_recorder.track_module(\"main_model\", model)\n",
    "\n",
    "# See https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html.\n",
    "REPORT_EVERY = 10\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "        b, _ = x.shape\n",
    "        h0 = torch.zeros(num_layers, b, hidden_size).to(device)\n",
    "        comgra.my_recorder.start_batch(epoch * 1000 + i, b) # TODO: figure out the correct way to calculate the training_step\n",
    "        comgra.my_recorder.start_iteration()\n",
    "\n",
    "        # See https://github.com/FlorianDietz/comgra?tab=readme-ov-file#known-issues.\n",
    "        comgra.my_recorder.register_tensor(\"inputs\", x.float(), is_input=True) # TODO: h0?\n",
    "        b = x * 1.0\n",
    "        b.requires_grad = True\n",
    "        comgra.my_recorder.add_tensor_connection(\"inputs\", b)\n",
    "        # Make predictions for this batch\n",
    "        y_pred, _ = model(x, h0)\n",
    "        comgra.my_recorder.register_tensor(\"outputs\", y_pred)\n",
    "        comgra.my_recorder.register_tensor(\"targets\", y.float(), is_target=True)\n",
    "        c = y * 1.0\n",
    "        comgra.my_recorder.add_tensor_connection(\"targets\", c)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "        comgra.my_recorder.register_tensor(\"loss\", loss, is_loss=True)\n",
    "        comgra.my_recorder.record_kpi_in_graph(\"loss\", \"\", loss)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        comgra.my_recorder.record_current_gradients(f\"gradients\")\n",
    "        comgra.my_recorder.finish_iteration()\n",
    "        comgra.my_recorder.finish_batch()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % REPORT_EVERY == REPORT_EVERY - 1:\n",
    "            last_loss = running_loss / REPORT_EVERY  # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "id": "df4ae777e1045f3c",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T07:29:07.911516Z",
     "start_time": "2025-07-15T07:26:25.252280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "best_val_loss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, val_data in enumerate(val_dataloader):\n",
    "            val_x, val_y = val_data\n",
    "            b, _ = val_x.shape\n",
    "            h0 = torch.zeros(num_layers, b, hidden_size)\n",
    "            val_y_pred, _ = model(val_x.to(device), h0.to(device))\n",
    "            val_loss = loss_fn(val_y_pred, val_y.to(device))\n",
    "            running_val_loss += val_loss\n",
    "\n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_val_loss))\n",
    "\n",
    "    # Track the best performance, and save the model's state\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_path = os.path.join(model_dir, 'model_{}_{}.pt'.format(timestamp, epoch_number))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "comgra.my_recorder.finalize()"
   ],
   "id": "d8c847828731f295",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.0847641348838806\n",
      "  batch 20 loss: 1.0678720355033875\n",
      "  batch 30 loss: 1.067219650745392\n",
      "  batch 40 loss: 1.0844271779060364\n",
      "  batch 50 loss: 1.0749964475631715\n",
      "  batch 60 loss: 1.0738522052764892\n",
      "  batch 70 loss: 1.066994273662567\n",
      "  batch 80 loss: 1.0814654231071472\n",
      "  batch 90 loss: 1.0740254402160645\n",
      "  batch 100 loss: 1.0746989727020264\n",
      "  batch 110 loss: 1.0760101079940796\n",
      "  batch 120 loss: 1.0711143732070922\n",
      "  batch 130 loss: 1.056877851486206\n",
      "  batch 140 loss: 1.0690032720565796\n",
      "  batch 150 loss: 1.079414188861847\n",
      "  batch 160 loss: 1.0485008716583253\n",
      "  batch 170 loss: 1.0582187533378602\n",
      "  batch 180 loss: 1.0567939162254334\n",
      "  batch 190 loss: 1.0566339492797852\n",
      "  batch 200 loss: 1.061097502708435\n",
      "  batch 210 loss: 1.0564129829406739\n",
      "  batch 220 loss: 1.0344078063964843\n",
      "  batch 230 loss: 1.0180722057819367\n",
      "  batch 240 loss: 1.0138613760471344\n",
      "LOSS train 1.0138613760471344 valid 1.0475882291793823\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 1.0016878426074982\n",
      "  batch 20 loss: 0.901980721950531\n",
      "  batch 30 loss: 0.9173439800739288\n",
      "  batch 40 loss: 0.9241255819797516\n",
      "  batch 50 loss: 0.8895343363285064\n",
      "  batch 60 loss: 0.9181353449821472\n",
      "  batch 70 loss: 0.8979746580123902\n",
      "  batch 80 loss: 0.8536453664302825\n",
      "  batch 90 loss: 0.8877518236637115\n",
      "  batch 100 loss: 0.8769648969173431\n",
      "  batch 110 loss: 0.8844407975673676\n",
      "  batch 120 loss: 0.88594491481781\n",
      "  batch 130 loss: 0.8777903199195862\n",
      "  batch 140 loss: 0.8216139137744903\n",
      "  batch 150 loss: 0.8780942916870117\n",
      "  batch 160 loss: 0.8450647592544556\n",
      "  batch 170 loss: 0.8720044434070587\n",
      "  batch 180 loss: 0.9001082062721253\n",
      "  batch 80 loss: 0.5129394650459289\n",
      "  batch 90 loss: 0.5952585518360138\n",
      "  batch 100 loss: 0.5542903572320939\n",
      "  batch 110 loss: 0.5961310923099518\n",
      "  batch 120 loss: 0.5862664639949798\n",
      "  batch 130 loss: 0.5469083964824677\n",
      "  batch 140 loss: 0.5592635422945023\n",
      "  batch 150 loss: 0.568945249915123\n",
      "  batch 160 loss: 0.6409348309040069\n",
      "  batch 170 loss: 0.6213276326656342\n",
      "  batch 180 loss: 0.5881140410900116\n",
      "  batch 190 loss: 0.6299967288970947\n",
      "  batch 200 loss: 0.5927373498678208\n",
      "  batch 210 loss: 0.6374647498130799\n",
      "  batch 220 loss: 0.6832893192768097\n",
      "  batch 230 loss: 0.6954437673091889\n",
      "  batch 240 loss: 0.7160207748413085\n",
      "LOSS train 0.7160207748413085 valid 0.7910531163215637\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:50:16.111162Z",
     "start_time": "2025-07-14T21:50:16.034368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Model(len(vocabulary), embedding_dim, hidden_size, num_layers).to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(os.getcwd(), 'model_20250715_004909_4'), map_location=torch.device(device)))"
   ],
   "id": "993f5e36681145a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T22:04:03.808056Z",
     "start_time": "2025-07-14T22:03:47.456312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Measuring the model performance\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for index in range(len(test_dataset_huggingface)):\n",
    "    h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "    message = test_dataset_huggingface['text'][index]\n",
    "    label = test_dataset_huggingface['label'][index]\n",
    "    encoded_message = encode_x(token_to_index, test_tokenized_messages[index])\n",
    "    encoded_label = encode_y(label)\n",
    "    x_batch, y_batch = torch.unsqueeze(encoded_message, dim=0), torch.unsqueeze(encoded_label, dim=0)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    # https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network\n",
    "    _, top_i = torch.topk(prediction, k=1)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    if labels[label] == labels[top_i[0].item()]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    if index % 100 == 0:\n",
    "        print(f'Finished test {index}, accuracy is {correct / total}')\n",
    "\n",
    "print(correct, total)\n",
    "model.train()"
   ],
   "id": "8b99638d25231845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished test 0, accuracy is 0.0\n",
      "Finished test 100, accuracy is 0.5742574257425742\n",
      "Finished test 200, accuracy is 0.6616915422885572\n",
      "Finished test 300, accuracy is 0.6677740863787376\n",
      "Finished test 400, accuracy is 0.683291770573566\n",
      "Finished test 500, accuracy is 0.6806387225548902\n",
      "Finished test 600, accuracy is 0.6905158069883528\n",
      "Finished test 700, accuracy is 0.7004279600570613\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[119]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m _, top_i = torch.topk(prediction, k=\u001B[32m1\u001B[39m)\n\u001B[32m     15\u001B[39m labels = [\u001B[33m'\u001B[39m\u001B[33mnegative\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mneutral\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mpositive\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m labels[label] == labels[\u001B[43mtop_i\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m]:\n\u001B[32m     17\u001B[39m     correct += \u001B[32m1\u001B[39m\n\u001B[32m     18\u001B[39m total += \u001B[32m1\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T22:02:22.307485Z",
     "start_time": "2025-07-14T22:02:22.273174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I never hated you\"\n",
    "message = \"I do not hate you\"\n",
    "message = \"I'm sick of that\"\n",
    "message = \"My computer is great\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch = torch.unsqueeze(encoded_message, dim=0)\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "print(prediction)\n",
    "_, top_i = torch.topk(prediction, k=1)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[top_i[0].item()])\n",
    "model.train()"
   ],
   "id": "db55682b7562318",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 206.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8819, -1.8181, -0.8588]], device='mps:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(36633, 4)\n",
       "  (rnn): GRU(4, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33a710c1218e59da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
