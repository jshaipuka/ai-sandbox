{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T15:44:09.694388Z",
     "start_time": "2025-07-01T15:44:09.690843Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:27:12.004196Z",
     "start_time": "2025-07-01T15:27:12.000193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:25.453103Z",
     "start_time": "2025-07-01T15:27:12.786138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in messages:\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = torch.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "validation_dataset: Dataset = dataset['validation'].filter(lambda it: len(it['text']) <= 128).map(process).sort(\n",
    "    'length')\n",
    "test_dataset: Dataset = dataset['test']\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset['text'])\n",
    "validation_tokens, validation_tokenized_messages = tokenize_messages(validation_dataset['text'])\n",
    "\n",
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae7f43e1f79849ddbd3b08c6609af1ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "043e19b54fea41419798b4cf5607cd32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a3f0d71382e4d89acddb52bfb7c1ef6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4279 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f8d75e56c954f6290b722e7bee7fd5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t ', '\\n', ' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ', '             ', '              ', '               ', '                ', '                                                                                              ', '!', '\"', '#', '$', '%']\n",
      "27438\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:31.657963Z",
     "start_time": "2025-07-01T15:28:31.644706Z"
    }
   },
   "cell_type": "code",
   "source": "token_to_index = {u: i for i, u in enumerate(vocabulary)}",
   "id": "1afd5c566494b4ee",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:32.945854Z",
     "start_time": "2025-07-01T15:28:32.624353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "validation_messages = [encode_x(token_to_index, tokens) for tokens in validation_tokenized_messages]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(validation_messages))\n",
    "print(len(validation_labels))"
   ],
   "id": "76c44cb0c0b867a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25913\n",
      "25913\n",
      "4279\n",
      "4279\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:29:57.954505Z",
     "start_time": "2025-07-01T15:29:57.880466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 20000\n",
    "print(train_dataset['text'][index], train_tokenized_messages[index])"
   ],
   "id": "5d91d7d27be6a117",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least he`s in breakthrough performance tho. I just wanted him nominated in his own category ['at', 'least', 'he`s', 'in', 'breakthrough', 'performance', 'tho', '.', 'i', 'just', 'wanted', 'him', 'nominated', 'in', 'his', 'own', 'category']\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:30:14.114707Z",
     "start_time": "2025-07-01T15:30:14.109577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size, padding_value):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True,\n",
    "                                     padding_value=padding_value).int(), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "3f23bbdb9a1ce2ad",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:41:18.971689Z",
     "start_time": "2025-07-01T15:41:18.966268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def estimate_loss(model, h0, iterations, validation_xs, validation_ys, batch_size, padding_value):\n",
    "    model.eval()\n",
    "    loses = torch.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size, padding_value)\n",
    "        validation_prediction, _ = model(validation_x_batch.to(device), h0.to(device))\n",
    "        validation_loss = F.cross_entropy(validation_prediction, validation_y_batch.to(device), reduction='mean')\n",
    "        loses[i] = validation_loss.item()\n",
    "    model.train()\n",
    "    return loses.mean()"
   ],
   "id": "c18ad73120373ebc",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:35:09.718910Z",
     "start_time": "2025-07-01T15:35:09.713201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        prediction_with_activation = prediction[:, -1, :]\n",
    "        return self.linear(prediction_with_activation), hidden"
   ],
   "id": "74d00197ac7e8079",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:45:38.114146Z",
     "start_time": "2025-07-01T15:44:11.501934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# TODO: try different batch sizes\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "embedding_dim = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 1000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_messages, train_labels, batch_size, len(vocabulary))\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == number_of_epoches - 1:\n",
    "        validation_loss = estimate_loss(model, torch.zeros(num_layers, batch_size, hidden_size), 32,\n",
    "                                        validation_messages, validation_labels, batch_size, len(vocabulary))\n",
    "        print(f'Epoch {epoch}, train loss {loss.item()}, validation loss {validation_loss.item()}')\n",
    "        if validation_loss < min_validation_loss:\n",
    "            model_file_name = os.path.join(model_dir, \"model_\" + str(epoch) + \".pt\")\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print(\"Model has been saved as\", model_file_name)\n",
    "            min_validation_loss = validation_loss\n"
   ],
   "id": "272aafaf0962bf85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 1.1001501083374023, validation loss 1.6942925453186035\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_0.pt\n",
      "Epoch 10, train loss 1.1714333295822144, validation loss 1.1617904901504517\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_10.pt\n",
      "Epoch 20, train loss 1.231337070465088, validation loss 1.225268006324768\n",
      "Epoch 30, train loss 1.1166577339172363, validation loss 1.1504684686660767\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_30.pt\n",
      "Epoch 40, train loss 1.1084315776824951, validation loss 1.0683907270431519\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_40.pt\n",
      "Epoch 50, train loss 0.8870217800140381, validation loss 1.1332043409347534\n",
      "Epoch 60, train loss 1.0806005001068115, validation loss 1.032279133796692\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_60.pt\n",
      "Epoch 70, train loss 0.9950116872787476, validation loss 1.0183320045471191\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_70.pt\n",
      "Epoch 80, train loss 0.875148594379425, validation loss 0.9619811773300171\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_80.pt\n",
      "Epoch 90, train loss 0.6829431653022766, validation loss 0.928175687789917\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_90.pt\n",
      "Epoch 100, train loss 0.7904233932495117, validation loss 0.8758301138877869\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_100.pt\n",
      "Epoch 110, train loss 0.7146521806716919, validation loss 0.9864858388900757\n",
      "Epoch 120, train loss 0.4685760736465454, validation loss 0.873145341873169\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_120.pt\n",
      "Epoch 130, train loss 0.5344574451446533, validation loss 0.8379389643669128\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_130.pt\n",
      "Epoch 140, train loss 0.32100218534469604, validation loss 0.9400031566619873\n",
      "Epoch 150, train loss 0.9502808451652527, validation loss 0.8101143836975098\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_150.pt\n",
      "Epoch 160, train loss 0.615822970867157, validation loss 0.8726922869682312\n",
      "Epoch 170, train loss 0.31961870193481445, validation loss 0.9351476430892944\n",
      "Epoch 180, train loss 0.340378075838089, validation loss 0.8103686571121216\n",
      "Epoch 190, train loss 0.6916157007217407, validation loss 0.9551498889923096\n",
      "Epoch 200, train loss 0.6048600673675537, validation loss 0.8762599229812622\n",
      "Epoch 210, train loss 0.48552531003952026, validation loss 0.9025804400444031\n",
      "Epoch 220, train loss 0.14295387268066406, validation loss 1.0164254903793335\n",
      "Epoch 230, train loss 0.39039385318756104, validation loss 0.8985078930854797\n",
      "Epoch 240, train loss 0.47628054022789, validation loss 0.8955921530723572\n",
      "Epoch 250, train loss 0.34407153725624084, validation loss 0.7817429304122925\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_250.pt\n",
      "Epoch 260, train loss 0.8885917663574219, validation loss 0.8099035024642944\n",
      "Epoch 270, train loss 0.630886435508728, validation loss 0.8776369094848633\n",
      "Epoch 280, train loss 0.2480008602142334, validation loss 0.8388245105743408\n",
      "Epoch 290, train loss 0.16364814341068268, validation loss 0.8718933463096619\n",
      "Epoch 300, train loss 0.11595600843429565, validation loss 0.9557672142982483\n",
      "Epoch 310, train loss 0.20630329847335815, validation loss 0.9658945798873901\n",
      "Epoch 320, train loss 0.54093998670578, validation loss 0.9020589590072632\n",
      "Epoch 330, train loss 0.43829888105392456, validation loss 0.8851633071899414\n",
      "Epoch 340, train loss 0.07230854034423828, validation loss 0.8962136507034302\n",
      "Epoch 350, train loss 0.72690749168396, validation loss 0.8689993023872375\n",
      "Epoch 360, train loss 0.7820924520492554, validation loss 0.9007022976875305\n",
      "Epoch 370, train loss 0.2549396753311157, validation loss 1.0183922052383423\n",
      "Epoch 380, train loss 0.3617627024650574, validation loss 0.8988040685653687\n",
      "Epoch 390, train loss 0.1508386731147766, validation loss 0.9583847522735596\n",
      "Epoch 400, train loss 0.1579209268093109, validation loss 1.0946192741394043\n",
      "Epoch 410, train loss 0.19724346697330475, validation loss 1.187138319015503\n",
      "Epoch 420, train loss 0.2521253228187561, validation loss 1.0457847118377686\n",
      "Epoch 430, train loss 0.11169980466365814, validation loss 1.1143094301223755\n",
      "Epoch 440, train loss 0.2802572548389435, validation loss 1.0643677711486816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[235]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     17\u001B[39m x_batch, y_batch = create_batch(train_messages, train_labels, batch_size, \u001B[38;5;28mlen\u001B[39m(vocabulary))\n\u001B[32m     18\u001B[39m h0 = torch.zeros(num_layers, batch_size, hidden_size)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m prediction, _ = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh0\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m loss = loss_fn(prediction, y_batch.to(device))\n\u001B[32m     22\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1738\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1740\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1750\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1753\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1754\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[222]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mModel.forward\u001B[39m\u001B[34m(self, sequence, hidden)\u001B[39m\n\u001B[32m     13\u001B[39m embedded = \u001B[38;5;28mself\u001B[39m.embedding(sequence)\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# (batch_size, sequence_length, embedding_dim)\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m prediction, hidden = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\u001B[39;00m\n\u001B[32m     18\u001B[39m prediction_with_activation = prediction[:, -\u001B[32m1\u001B[39m, :]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1738\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1740\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1750\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1753\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1754\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1391\u001B[39m, in \u001B[36mGRU.forward\u001B[39m\u001B[34m(self, input, hx)\u001B[39m\n\u001B[32m   1389\u001B[39m \u001B[38;5;28mself\u001B[39m.check_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[32m   1390\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1391\u001B[39m     result = \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgru\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1394\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1395\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1396\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1397\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1398\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1399\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1400\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1401\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1402\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1403\u001B[39m     result = _VF.gru(\n\u001B[32m   1404\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   1405\u001B[39m         batch_sizes,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1412\u001B[39m         \u001B[38;5;28mself\u001B[39m.bidirectional,\n\u001B[32m   1413\u001B[39m     )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:46:03.104512Z",
     "start_time": "2025-07-01T15:46:02.698133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(os.getcwd(), \"models\", 'model_250.pt'), map_location=torch.device(device)))"
   ],
   "id": "d2a5b9f67db1eb28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:47:02.525623Z",
     "start_time": "2025-07-01T15:47:02.465367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "index = 1034\n",
    "message = validation_dataset['text'][index]\n",
    "label = validation_dataset['label'][index]\n",
    "print(message)\n",
    "encoded_message = encode_x(token_to_index, validation_tokenized_messages[index])\n",
    "encoded_label = encode_y(label)\n",
    "x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[label])\n",
    "print(labels[torch.argmax(distribution)], distribution)"
   ],
   "id": "e2727732916a2a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking mum to lunch for mothers day\n",
      "neutral\n",
      "neutral tensor([[0.0171, 0.9147, 0.0682]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:26:26.611892Z",
     "start_time": "2025-07-01T15:26:26.490985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I hated you\"  # \"I never hated you\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch, _ = create_batch([encoded_message], [torch.tensor([0, 0, 0])], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "print(distribution)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[torch.argmax(distribution)])"
   ],
   "id": "8ec28040b1a09fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9808, 0.0122, 0.0070]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "negative\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bba20ab64630ba51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
