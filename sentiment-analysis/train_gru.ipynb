{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T15:44:09.694388Z",
     "start_time": "2025-07-01T15:44:09.690843Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:27:12.004196Z",
     "start_time": "2025-07-01T15:27:12.000193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:25.453103Z",
     "start_time": "2025-07-01T15:27:12.786138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in messages:\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = torch.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "validation_dataset: Dataset = dataset['validation'].filter(lambda it: len(it['text']) <= 128).map(process).sort(\n",
    "    'length')\n",
    "test_dataset: Dataset = dataset['test']\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset['text'])\n",
    "validation_tokens, validation_tokenized_messages = tokenize_messages(validation_dataset['text'])\n",
    "\n",
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae7f43e1f79849ddbd3b08c6609af1ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "043e19b54fea41419798b4cf5607cd32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a3f0d71382e4d89acddb52bfb7c1ef6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4279 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f8d75e56c954f6290b722e7bee7fd5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t ', '\\n', ' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ', '             ', '              ', '               ', '                ', '                                                                                              ', '!', '\"', '#', '$', '%']\n",
      "27438\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:31.657963Z",
     "start_time": "2025-07-01T15:28:31.644706Z"
    }
   },
   "cell_type": "code",
   "source": "token_to_index = {u: i for i, u in enumerate(vocabulary)}",
   "id": "1afd5c566494b4ee",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:28:32.945854Z",
     "start_time": "2025-07-01T15:28:32.624353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "validation_messages = [encode_x(token_to_index, tokens) for tokens in validation_tokenized_messages]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(validation_messages))\n",
    "print(len(validation_labels))"
   ],
   "id": "76c44cb0c0b867a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25913\n",
      "25913\n",
      "4279\n",
      "4279\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:29:57.954505Z",
     "start_time": "2025-07-01T15:29:57.880466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 20000\n",
    "print(train_dataset['text'][index], train_tokenized_messages[index])"
   ],
   "id": "5d91d7d27be6a117",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least he`s in breakthrough performance tho. I just wanted him nominated in his own category ['at', 'least', 'he`s', 'in', 'breakthrough', 'performance', 'tho', '.', 'i', 'just', 'wanted', 'him', 'nominated', 'in', 'his', 'own', 'category']\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:30:14.114707Z",
     "start_time": "2025-07-01T15:30:14.109577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size, padding_value):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True,\n",
    "                                     padding_value=padding_value).int(), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "3f23bbdb9a1ce2ad",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:41:18.971689Z",
     "start_time": "2025-07-01T15:41:18.966268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def estimate_loss(model, h0, iterations, validation_xs, validation_ys, batch_size, padding_value):\n",
    "    model.eval()\n",
    "    loses = torch.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size, padding_value)\n",
    "        validation_prediction, _ = model(validation_x_batch.to(device), h0.to(device))\n",
    "        validation_loss = F.cross_entropy(validation_prediction, validation_y_batch.to(device), reduction='mean')\n",
    "        loses[i] = validation_loss.item()\n",
    "    model.train()\n",
    "    return loses.mean()"
   ],
   "id": "c18ad73120373ebc",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T16:06:46.781405Z",
     "start_time": "2025-07-01T16:06:46.774057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        return self.linear(prediction[:,-1]), hidden"
   ],
   "id": "74d00197ac7e8079",
   "outputs": [],
   "execution_count": 273
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T16:07:17.626896Z",
     "start_time": "2025-07-01T16:06:47.085921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "embedding_dim = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 1000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_messages, train_labels, batch_size, len(vocabulary))\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == number_of_epoches - 1:\n",
    "        validation_loss = estimate_loss(model, torch.zeros(num_layers, batch_size, hidden_size), 32,\n",
    "                                        validation_messages, validation_labels, batch_size, len(vocabulary))\n",
    "        print(f'Epoch {epoch}, train loss {loss.item()}, validation loss {validation_loss.item()}')\n",
    "        if validation_loss < min_validation_loss:\n",
    "            model_file_name = os.path.join(model_dir, \"model_\" + str(epoch) + \".pt\")\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print(\"Model has been saved as\", model_file_name)\n",
    "            min_validation_loss = validation_loss\n"
   ],
   "id": "272aafaf0962bf85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 1.1021161079406738, validation loss 1.7978289127349854\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_0.pt\n",
      "Epoch 10, train loss 1.7316954135894775, validation loss 1.236612319946289\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_10.pt\n",
      "Epoch 20, train loss 0.9763991832733154, validation loss 1.2400579452514648\n",
      "Epoch 30, train loss 1.0728509426116943, validation loss 1.0749861001968384\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_30.pt\n",
      "Epoch 40, train loss 1.0888936519622803, validation loss 1.1237928867340088\n",
      "Epoch 50, train loss 1.0700898170471191, validation loss 1.0551788806915283\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_50.pt\n",
      "Epoch 60, train loss 0.7927769422531128, validation loss 1.0915335416793823\n",
      "Epoch 70, train loss 0.8287042379379272, validation loss 1.0858982801437378\n",
      "Epoch 80, train loss 0.9265398979187012, validation loss 1.0509564876556396\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_80.pt\n",
      "Epoch 90, train loss 0.9703265428543091, validation loss 0.9325956106185913\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_90.pt\n",
      "Epoch 100, train loss 0.6400607228279114, validation loss 0.8902565836906433\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_100.pt\n",
      "Epoch 110, train loss 0.9555523991584778, validation loss 0.9277031421661377\n",
      "Epoch 120, train loss 0.3461728096008301, validation loss 0.9215936660766602\n",
      "Epoch 130, train loss 0.7516986131668091, validation loss 0.9499308466911316\n",
      "Epoch 140, train loss 0.6725128889083862, validation loss 0.8992571830749512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[274]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     23\u001B[39m model.zero_grad()\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m epoch % \u001B[32m10\u001B[39m == \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m epoch == number_of_epoches - \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     validation_loss = \u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m                                    \u001B[49m\u001B[43mvalidation_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss.item()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, validation loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalidation_loss.item()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m validation_loss < min_validation_loss:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[230]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mestimate_loss\u001B[39m\u001B[34m(model, h0, iterations, validation_xs, validation_ys, batch_size, padding_value)\u001B[39m\n\u001B[32m      5\u001B[39m     validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size, padding_value)\n\u001B[32m      6\u001B[39m     validation_prediction, _ = model(validation_x_batch.to(device), h0.to(device))\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     validation_loss = F.cross_entropy(validation_prediction, \u001B[43mvalidation_y_batch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, reduction=\u001B[33m'\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      8\u001B[39m     loses[i] = validation_loss.item()\n\u001B[32m      9\u001B[39m model.train()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 274
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T16:07:21.479516Z",
     "start_time": "2025-07-01T16:07:21.330175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(os.getcwd(), \"models\", 'model_100.pt'), map_location=torch.device(device)))"
   ],
   "id": "d2a5b9f67db1eb28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 275
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T16:07:31.860579Z",
     "start_time": "2025-07-01T16:07:21.505482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for index in range(len(validation_dataset)):\n",
    "    h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "    message = validation_dataset['text'][index]\n",
    "    label = validation_dataset['label'][index]\n",
    "    encoded_message = encode_x(token_to_index, validation_tokenized_messages[index])\n",
    "    encoded_label = encode_y(label)\n",
    "    x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1, len(vocabulary))\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    if labels[label] == labels[torch.argmax(distribution)]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    if index % 100 == 0:\n",
    "        print(f'Finished test {index}, accuracy is {correct / total}')\n",
    "\n",
    "print(correct, total)"
   ],
   "id": "e2727732916a2a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished test 0, accuracy is 0.0\n",
      "Finished test 100, accuracy is 0.594059405940594\n",
      "Finished test 200, accuracy is 0.6766169154228856\n",
      "Finished test 300, accuracy is 0.6677740863787376\n",
      "Finished test 400, accuracy is 0.6583541147132169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[276]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m h0 = torch.zeros(num_layers, \u001B[32m1\u001B[39m, hidden_size)\n\u001B[32m      5\u001B[39m message = validation_dataset[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m][index]\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m label = \u001B[43mvalidation_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlabel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[index]\n\u001B[32m      7\u001B[39m encoded_message = encode_x(token_to_index, validation_tokenized_messages[index])\n\u001B[32m      8\u001B[39m encoded_label = encode_y(label)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001B[39m, in \u001B[36mDataset.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   2775\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[32m   2776\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2777\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2761\u001B[39m, in \u001B[36mDataset._getitem\u001B[39m\u001B[34m(self, key, **kwargs)\u001B[39m\n\u001B[32m   2759\u001B[39m format_kwargs = format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[32m   2760\u001B[39m formatter = get_formatter(format_type, features=\u001B[38;5;28mself\u001B[39m._info.features, **format_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m2761\u001B[39m pa_subtable = \u001B[43mquery_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2762\u001B[39m formatted_output = format_table(\n\u001B[32m   2763\u001B[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001B[32m   2764\u001B[39m )\n\u001B[32m   2765\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:612\u001B[39m, in \u001B[36mquery_table\u001B[39m\u001B[34m(table, key, indices)\u001B[39m\n\u001B[32m    610\u001B[39m     pa_subtable = _query_table(table, key)\n\u001B[32m    611\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m612\u001B[39m     pa_subtable = \u001B[43m_query_table_with_indices_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    613\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pa_subtable\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:72\u001B[39m, in \u001B[36m_query_table_with_indices_mapping\u001B[39m\u001B[34m(table, key, indices)\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     71\u001B[39m     table = table.select([key])\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_query_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_pylist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Iterable):\n\u001B[32m     74\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _query_table(table, [indices.fast_slice(i, \u001B[32m1\u001B[39m).column(\u001B[32m0\u001B[39m)[\u001B[32m0\u001B[39m].as_py() \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m key])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:99\u001B[39m, in \u001B[36m_query_table\u001B[39m\u001B[34m(table, key)\u001B[39m\n\u001B[32m     97\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m table.table.slice(\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m)\n\u001B[32m     98\u001B[39m     \u001B[38;5;66;03m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfast_gather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[43m%\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnum_rows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m _raise_bad_key_type(key)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/table.py:124\u001B[39m, in \u001B[36mIndexedTableMixin.fast_gather\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mIndices must be non-empty\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    121\u001B[39m batch_indices = np.searchsorted(\u001B[38;5;28mself\u001B[39m._offsets, indices, side=\u001B[33m\"\u001B[39m\u001B[33mright\u001B[39m\u001B[33m\"\u001B[39m) - \u001B[32m1\u001B[39m\n\u001B[32m    122\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pa.Table.from_batches(\n\u001B[32m    123\u001B[39m     [\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mslice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_offsets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    125\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(batch_indices, indices)\n\u001B[32m    126\u001B[39m     ],\n\u001B[32m    127\u001B[39m     schema=\u001B[38;5;28mself\u001B[39m._schema,\n\u001B[32m    128\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T15:26:26.611892Z",
     "start_time": "2025-07-01T15:26:26.490985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I hated you\"  # \"I never hated you\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch, _ = create_batch([encoded_message], [torch.tensor([0, 0, 0])], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "distribution = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "print(distribution)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[torch.argmax(distribution)])"
   ],
   "id": "8ec28040b1a09fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9808, 0.0122, 0.0070]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "negative\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bba20ab64630ba51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
