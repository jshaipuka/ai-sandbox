{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:40:24.734457Z",
     "start_time": "2025-07-05T08:40:24.730650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ],
   "id": "a9dea8914236e91",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:40:25.163616Z",
     "start_time": "2025-07-05T08:40:25.159108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "cc76c8b605ea8925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:04.811735Z",
     "start_time": "2025-07-05T08:44:04.806985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in tqdm(messages):\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "# Returns a scalar with the class index (0, 1 or 2).\n",
    "def encode_y(label):\n",
    "    return torch.tensor(label)"
   ],
   "id": "163dd410cddbc3f7",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:41:53.326171Z",
     "start_time": "2025-07-05T08:40:27.768642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "validation_dataset: Dataset = dataset['validation'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "test_dataset: Dataset = dataset['test'].filter(lambda it: it['text'] is not None and len(it['text']) <= 128).map(process).sort('length')\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset['text'])\n",
    "validation_tokens, validation_tokenized_messages = tokenize_messages(validation_dataset['text'])\n",
    "test_tokens, test_tokenized_messages = tokenize_messages(test_dataset['text'])"
   ],
   "id": "6f85cdf981c20f0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c50c800f54e64370bfff2eb589193045"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71d163de8dfe478ea1931fe7229d6ee5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "496fcfc7c9604fcb923b286d6a6d9abe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4279 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82592a86279c4b87bec150151c07356e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5206 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80ab9029ffe7489da75e596245da84f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4341 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02c8c84988ba4cbcb9e62b053e2a5d93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25913/25913 [01:00<00:00, 426.53it/s]\n",
      "100%|██████████| 4279/4279 [00:09<00:00, 428.37it/s]\n",
      "100%|██████████| 4341/4341 [00:10<00:00, 425.57it/s]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:41:57.845375Z",
     "start_time": "2025-07-05T08:41:57.814318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens + test_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))\n",
    "\n",
    "token_to_index = {u: i for i, u in enumerate(vocabulary)}"
   ],
   "id": "83a48a58a92b318c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t ', '\\n', ' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ', '             ', '              ', '               ', '                ', '                                                                                              ', '!', '\"', '#', '$', '%']\n",
      "29886\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:09.350752Z",
     "start_time": "2025-07-05T08:44:09.052435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "validation_messages = [encode_x(token_to_index, tokens) for tokens in validation_tokenized_messages]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset['label']]\n",
    "test_messages = [encode_x(token_to_index, tokens) for tokens in test_tokenized_messages]\n",
    "test_labels = [encode_y(label) for label in test_dataset['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(validation_messages))\n",
    "print(len(validation_labels))\n",
    "print(len(test_messages))\n",
    "print(len(test_labels))"
   ],
   "id": "890b7a1117c53c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25913\n",
      "25913\n",
      "4279\n",
      "4279\n",
      "4341\n",
      "4341\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:10.527093Z",
     "start_time": "2025-07-05T08:44:10.471165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 20000\n",
    "print(train_dataset['text'][index], train_tokenized_messages[index])"
   ],
   "id": "5377b20cad482166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least he`s in breakthrough performance tho. I just wanted him nominated in his own category ['at', 'least', 'he`s', 'in', 'breakthrough', 'performance', 'tho', '.', 'i', 'just', 'wanted', 'him', 'nominated', 'in', 'his', 'own', 'category']\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:12.231850Z",
     "start_time": "2025-07-05T08:44:12.228910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size, padding_value):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True,\n",
    "                                     padding_value=padding_value).int(), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "b3d48b570cd093b7",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:12.813758Z",
     "start_time": "2025-07-05T08:44:12.809376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def estimate_loss(model, h0, iterations, validation_xs, validation_ys, batch_size, padding_value):\n",
    "    model.eval()\n",
    "    loses = torch.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size, padding_value)\n",
    "        validation_prediction, _ = model(validation_x_batch.to(device), h0.to(device))\n",
    "        validation_loss = F.nll_loss(validation_prediction, validation_y_batch.to(device))\n",
    "        loses[i] = validation_loss.item()\n",
    "    model.train()\n",
    "    return loses.mean()"
   ],
   "id": "ae850c6100343752",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:44:13.653245Z",
     "start_time": "2025-07-05T08:44:13.649652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1) # The negative log likelihood loss expects log-probabilities of each class.\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        linear_prediction = self.linear(prediction[:, -1])\n",
    "        return self.log_softmax(linear_prediction), hidden"
   ],
   "id": "60774ac30b3db966",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T08:58:54.972480Z",
     "start_time": "2025-07-05T08:52:05.193553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 256\n",
    "# See https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2.\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "embedding_dim = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 1000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_messages, train_labels, batch_size, len(vocabulary))\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == number_of_epoches - 1:\n",
    "        validation_loss = estimate_loss(model, torch.zeros(num_layers, batch_size, hidden_size), 32,\n",
    "                                        validation_messages, validation_labels, batch_size, len(vocabulary))\n",
    "        print(f'Epoch {epoch}, train loss {loss.item()}, validation loss {validation_loss.item()}')\n",
    "        if validation_loss < min_validation_loss:\n",
    "            model_file_name = os.path.join(model_dir, \"model_\" + str(epoch) + \".pt\")\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print(\"Model has been saved as\", model_file_name)\n",
    "            min_validation_loss = validation_loss\n"
   ],
   "id": "57891436daa9d9f4",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     17\u001B[39m h0 = torch.zeros(num_layers, batch_size, hidden_size)\n\u001B[32m     18\u001B[39m prediction, _ = model(x_batch.to(device), h0.to(device))\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m loss = loss_fn(prediction, y_batch.to(device))\n\u001B[32m     21\u001B[39m loss.backward()\n\u001B[32m     22\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1187\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.SafeCallWrapper.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:627\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:937\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:928\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:585\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.do_wait_suspend\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:00.657303Z",
     "start_time": "2025-07-05T09:03:00.549487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(os.getcwd(), \"models\", 'model_340.pt'), map_location=torch.device(device)))"
   ],
   "id": "993f5e36681145a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:08:15.152353Z",
     "start_time": "2025-07-05T09:08:09.369478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Measuring the model performance\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for index in range(len(test_dataset)):\n",
    "    h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "    message = test_dataset['text'][index]\n",
    "    label = test_dataset['label'][index]\n",
    "    encoded_message = encode_x(token_to_index, test_tokenized_messages[index])\n",
    "    encoded_label = encode_y(label)\n",
    "    x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1, len(vocabulary))\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    # https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network\n",
    "    _, top_i = torch.topk(prediction, k=1)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    if labels[label] == labels[top_i[0].item()]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    if index % 100 == 0:\n",
    "        print(f'Finished test {index}, accuracy is {correct / total}')\n",
    "\n",
    "print(correct, total)\n",
    "model.train()"
   ],
   "id": "8b99638d25231845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished test 0, accuracy is 1.0\n",
      "Finished test 100, accuracy is 0.5841584158415841\n",
      "Finished test 200, accuracy is 0.6716417910447762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[111]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(test_dataset)):\n\u001B[32m      6\u001B[39m     h0 = torch.zeros(num_layers, \u001B[32m1\u001B[39m, hidden_size)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     message = \u001B[43mtest_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtext\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[index]\n\u001B[32m      8\u001B[39m     label = test_dataset[\u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m][index]\n\u001B[32m      9\u001B[39m     encoded_message = encode_x(token_to_index, test_tokenized_messages[index])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001B[39m, in \u001B[36mDataset.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   2775\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[32m   2776\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2777\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001B[39m, in \u001B[36mDataset._getitem\u001B[39m\u001B[34m(self, key, **kwargs)\u001B[39m\n\u001B[32m   2760\u001B[39m formatter = get_formatter(format_type, features=\u001B[38;5;28mself\u001B[39m._info.features, **format_kwargs)\n\u001B[32m   2761\u001B[39m pa_subtable = query_table(\u001B[38;5;28mself\u001B[39m._data, key, indices=\u001B[38;5;28mself\u001B[39m._indices)\n\u001B[32m-> \u001B[39m\u001B[32m2762\u001B[39m formatted_output = \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2763\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpa_subtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_all_columns\u001B[49m\n\u001B[32m   2764\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2765\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:653\u001B[39m, in \u001B[36mformat_table\u001B[39m\u001B[34m(table, key, formatter, format_columns, output_all_columns)\u001B[39m\n\u001B[32m    651\u001B[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001B[32m    652\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m653\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    654\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m query_type == \u001B[33m\"\u001B[39m\u001B[33mcolumn\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    655\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:408\u001B[39m, in \u001B[36mFormatter.__call__\u001B[39m\u001B[34m(self, pa_table, query_type)\u001B[39m\n\u001B[32m    406\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.format_row(pa_table)\n\u001B[32m    407\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m query_type == \u001B[33m\"\u001B[39m\u001B[33mcolumn\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m408\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m query_type == \u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    410\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.format_batch(pa_table)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:459\u001B[39m, in \u001B[36mPythonFormatter.format_column\u001B[39m\u001B[34m(self, pa_table)\u001B[39m\n\u001B[32m    458\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa.Table) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     column = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpython_arrow_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mextract_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    460\u001B[39m     column = \u001B[38;5;28mself\u001B[39m.python_features_decoder.decode_column(column, pa_table.column_names[\u001B[32m0\u001B[39m])\n\u001B[32m    461\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m column\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:146\u001B[39m, in \u001B[36mPythonArrowExtractor.extract_column\u001B[39m\u001B[34m(self, pa_table)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mextract_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa.Table) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpa_table\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_pylist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:06:01.297841Z",
     "start_time": "2025-07-05T09:06:01.226796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I go home\"  # \"I never hated you\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch, _ = create_batch([encoded_message], [torch.tensor([0, 0, 0])], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "print(prediction)\n",
    "_, top_i = torch.topk(prediction, k=1)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[top_i[0].item()])\n",
    "model.train()"
   ],
   "id": "db55682b7562318",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 301.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6928, -0.3067, -2.5241]], device='mps:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(29886, 512)\n",
       "  (rnn): GRU(512, 1024, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33a710c1218e59da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
