{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-30T19:30:50.989463Z",
     "start_time": "2025-06-30T19:30:50.985909Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:30:51.293161Z",
     "start_time": "2025-06-30T19:30:51.287744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:31:57.981855Z",
     "start_time": "2025-06-30T19:31:55.568875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from util import BucketBatchSampler, BucketDataset\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train']\n",
    "validation_dataset: Dataset = dataset['validation']\n",
    "test_dataset: Dataset = dataset['test']\n",
    "\n",
    "corpus = concatenate_datasets([train_dataset, validation_dataset])['text']\n",
    "vocabulary = sorted(set(''.join(corpus)))\n",
    "\n",
    "char_to_i = {u: i for i, u in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def encode_x(char_to_i, message):\n",
    "    return np.array([char_to_i[char] for char in message])\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = torch.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "train_messages = [encode_x(char_to_i, message) for message in train_dataset['text']]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "\n",
    "train_bucket_batch_sampler = BucketBatchSampler(train_messages, 128)  # <-- does not store X\n",
    "train_bucket_dataset = BucketDataset(train_messages, train_labels)\n",
    "train_dataloader = DataLoader(train_bucket_dataset, batch_size=1, batch_sampler=train_bucket_batch_sampler, num_workers=8, drop_last=False)\n"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:49:47.985641Z",
     "start_time": "2025-06-30T19:49:47.980643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        return self.linear(hidden[0]), hidden"
   ],
   "id": "74d00197ac7e8079",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:51:02.127829Z",
     "start_time": "2025-06-30T19:49:49.136907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# See https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop.\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    x, y = data\n",
    "    batch_size = x.shape[0]\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    # See https://discuss.pytorch.org/t/gru-and-padded-sequences-tipps-and-tricks/90729.\n",
    "    prediction, _ = model.forward(x.to(device), h0)\n",
    "\n",
    "    loss = loss_fn(prediction, y.to(device))\n",
    "    print(f'Batch {i} of size {batch_size}, loss {loss.item()}')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ],
   "id": "4ab6bfa7cbf7355e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of size 128, loss 1.1001335382461548\n",
      "Batch 1 of size 8, loss 3.8926305770874023\n",
      "Batch 2 of size 90, loss 1.6588584184646606\n",
      "Batch 3 of size 26, loss 1.4633713960647583\n",
      "Batch 4 of size 128, loss 1.1869010925292969\n",
      "Batch 5 of size 103, loss 1.0807524919509888\n",
      "Batch 6 of size 1, loss 0.6781896948814392\n",
      "Batch 7 of size 16, loss 1.9421859979629517\n",
      "Batch 8 of size 128, loss 1.5011545419692993\n",
      "Batch 9 of size 4, loss 1.7138030529022217\n",
      "Batch 10 of size 7, loss 1.9737110137939453\n",
      "Batch 11 of size 128, loss 1.7998613119125366\n",
      "Batch 12 of size 1, loss 0.8564717769622803\n",
      "Batch 13 of size 128, loss 1.4678044319152832\n",
      "Batch 14 of size 5, loss 3.0721302032470703\n",
      "Batch 15 of size 19, loss 1.5426909923553467\n",
      "Batch 16 of size 128, loss 1.4457134008407593\n",
      "Batch 17 of size 8, loss 1.453172206878662\n",
      "Batch 18 of size 8, loss 1.4024332761764526\n",
      "Batch 19 of size 1, loss 2.130967140197754\n",
      "Batch 20 of size 36, loss 1.6565011739730835\n",
      "Batch 21 of size 1, loss 1.4566657543182373\n",
      "Batch 22 of size 128, loss 1.7772853374481201\n",
      "Batch 23 of size 1, loss 0.5182949900627136\n",
      "Batch 24 of size 20, loss 2.5169663429260254\n",
      "Batch 25 of size 8, loss 1.8316686153411865\n",
      "Batch 26 of size 128, loss 1.1442105770111084\n",
      "Batch 27 of size 5, loss 1.8505184650421143\n",
      "Batch 28 of size 1, loss 2.2398457527160645\n",
      "Batch 29 of size 4, loss 1.1278631687164307\n",
      "Batch 30 of size 1, loss 1.8773654699325562\n",
      "Batch 31 of size 23, loss 1.467028021812439\n",
      "Batch 32 of size 5, loss 2.2373948097229004\n",
      "Batch 33 of size 3, loss 1.085519790649414\n",
      "Batch 34 of size 17, loss 1.8889005184173584\n",
      "Batch 35 of size 2, loss 3.8675339221954346\n",
      "Batch 36 of size 128, loss 2.2848012447357178\n",
      "Batch 37 of size 25, loss 2.7214417457580566\n",
      "Batch 38 of size 16, loss 1.2915794849395752\n",
      "Batch 39 of size 20, loss 1.2363640069961548\n",
      "Batch 40 of size 6, loss 0.49216175079345703\n",
      "Batch 41 of size 4, loss 0.9735527634620667\n",
      "Batch 42 of size 79, loss 2.4096145629882812\n",
      "Batch 43 of size 128, loss 1.6369147300720215\n",
      "Batch 44 of size 1, loss 2.5619335174560547\n",
      "Batch 45 of size 3, loss 0.6023991703987122\n",
      "Batch 46 of size 1, loss 0.044162169098854065\n",
      "Batch 47 of size 102, loss 3.186033248901367\n",
      "Batch 48 of size 11, loss 4.570949077606201\n",
      "Batch 49 of size 5, loss 1.0931698083877563\n",
      "Batch 50 of size 10, loss 2.3152143955230713\n",
      "Batch 51 of size 6, loss 1.7913802862167358\n",
      "Batch 52 of size 3, loss 3.779078722000122\n",
      "Batch 53 of size 106, loss 1.5003412961959839\n",
      "Batch 54 of size 10, loss 2.373505115509033\n",
      "Batch 55 of size 1, loss 0.033252641558647156\n",
      "Batch 56 of size 8, loss 2.777735471725464\n",
      "Batch 57 of size 14, loss 2.0460805892944336\n",
      "Batch 58 of size 1, loss 0.10020323097705841\n",
      "Batch 59 of size 11, loss 1.5931099653244019\n",
      "Batch 60 of size 14, loss 1.374076008796692\n",
      "Batch 61 of size 14, loss 1.3527308702468872\n",
      "Batch 62 of size 27, loss 1.5598067045211792\n",
      "Batch 63 of size 4, loss 1.3038396835327148\n",
      "Batch 64 of size 3, loss 1.3032442331314087\n",
      "Batch 65 of size 13, loss 1.799689531326294\n",
      "Batch 66 of size 11, loss 1.3269755840301514\n",
      "Batch 67 of size 9, loss 1.4881500005722046\n",
      "Batch 68 of size 128, loss 1.4812633991241455\n",
      "Batch 69 of size 6, loss 0.6661744117736816\n",
      "Batch 70 of size 128, loss 1.4492911100387573\n",
      "Batch 71 of size 1, loss 3.22818922996521\n",
      "Batch 72 of size 17, loss 1.319992184638977\n",
      "Batch 73 of size 6, loss 1.7182823419570923\n",
      "Batch 74 of size 10, loss 1.3558629751205444\n",
      "Batch 75 of size 17, loss 1.4775680303573608\n",
      "Batch 76 of size 14, loss 2.118326187133789\n",
      "Batch 77 of size 1, loss 0.20829974114894867\n",
      "Batch 78 of size 1, loss 3.804694414138794\n",
      "Batch 79 of size 6, loss 1.2296286821365356\n",
      "Batch 80 of size 17, loss 1.234387993812561\n",
      "Batch 81 of size 105, loss 1.0995681285858154\n",
      "Batch 82 of size 8, loss 1.9199135303497314\n",
      "Batch 83 of size 6, loss 1.6516847610473633\n",
      "Batch 84 of size 128, loss 1.4457383155822754\n",
      "Batch 85 of size 3, loss 1.6970605850219727\n",
      "Batch 86 of size 17, loss 1.346129298210144\n",
      "Batch 87 of size 13, loss 1.1344794034957886\n",
      "Batch 88 of size 128, loss 1.3690606355667114\n",
      "Batch 89 of size 1, loss 4.557480812072754\n",
      "Batch 90 of size 54, loss 1.3247416019439697\n",
      "Batch 91 of size 11, loss 1.308573603630066\n",
      "Batch 92 of size 36, loss 1.34847891330719\n",
      "Batch 93 of size 128, loss 1.2490314245224\n",
      "Batch 94 of size 5, loss 1.5749202966690063\n",
      "Batch 95 of size 10, loss 1.5444060564041138\n",
      "Batch 96 of size 54, loss 1.099385142326355\n",
      "Batch 97 of size 1, loss 0.8486411571502686\n",
      "Batch 98 of size 9, loss 1.7546336650848389\n",
      "Batch 99 of size 128, loss 1.522292971611023\n",
      "Batch 100 of size 128, loss 1.4537924528121948\n",
      "Batch 101 of size 4, loss 0.8625357151031494\n",
      "Batch 102 of size 11, loss 1.8513588905334473\n",
      "Batch 103 of size 7, loss 2.228137493133545\n",
      "Batch 104 of size 128, loss 1.3657965660095215\n",
      "Batch 105 of size 20, loss 1.4344866275787354\n",
      "Batch 106 of size 123, loss 1.6178799867630005\n",
      "Batch 107 of size 1, loss 1.6323175430297852\n",
      "Batch 108 of size 31, loss 1.655181646347046\n",
      "Batch 109 of size 7, loss 1.56363844871521\n",
      "Batch 110 of size 17, loss 1.7381733655929565\n",
      "Batch 111 of size 17, loss 1.2833529710769653\n",
      "Batch 112 of size 1, loss 1.0706608295440674\n",
      "Batch 113 of size 128, loss 1.3693187236785889\n",
      "Batch 114 of size 7, loss 1.1267374753952026\n",
      "Batch 115 of size 11, loss 1.85529625415802\n",
      "Batch 116 of size 128, loss 1.733964204788208\n",
      "Batch 117 of size 128, loss 1.8299930095672607\n",
      "Batch 118 of size 128, loss 1.4421203136444092\n",
      "Batch 119 of size 5, loss 2.4834582805633545\n",
      "Batch 120 of size 128, loss 1.3882946968078613\n",
      "Batch 121 of size 86, loss 1.4415721893310547\n",
      "Batch 122 of size 8, loss 1.5028040409088135\n",
      "Batch 123 of size 87, loss 1.2081772089004517\n",
      "Batch 124 of size 120, loss 1.3485032320022583\n",
      "Batch 125 of size 128, loss 1.2042460441589355\n",
      "Batch 126 of size 1, loss 0.23899248242378235\n",
      "Batch 127 of size 128, loss 1.2894008159637451\n",
      "Batch 128 of size 65, loss 1.4328348636627197\n",
      "Batch 129 of size 128, loss 1.4378511905670166\n",
      "Batch 130 of size 43, loss 1.1400182247161865\n",
      "Batch 131 of size 10, loss 1.2317135334014893\n",
      "Batch 132 of size 45, loss 1.459402084350586\n",
      "Batch 133 of size 5, loss 1.088138222694397\n",
      "Batch 134 of size 8, loss 1.4739899635314941\n",
      "Batch 135 of size 76, loss 1.313473105430603\n",
      "Batch 136 of size 9, loss 1.2463908195495605\n",
      "Batch 137 of size 82, loss 1.1419249773025513\n",
      "Batch 138 of size 6, loss 1.284737229347229\n",
      "Batch 139 of size 2, loss 3.1245994567871094\n",
      "Batch 140 of size 1, loss 0.5231711268424988\n",
      "Batch 141 of size 36, loss 1.161518931388855\n",
      "Batch 142 of size 15, loss 2.0922014713287354\n",
      "Batch 143 of size 1, loss 1.7413725852966309\n",
      "Batch 144 of size 11, loss 1.6019020080566406\n",
      "Batch 145 of size 22, loss 2.2389113903045654\n",
      "Batch 146 of size 128, loss 1.8458542823791504\n",
      "Batch 147 of size 13, loss 3.3683359622955322\n",
      "Batch 148 of size 2, loss 1.2817268371582031\n",
      "Batch 149 of size 128, loss 1.3201727867126465\n",
      "Batch 150 of size 4, loss 1.0441465377807617\n",
      "Batch 151 of size 4, loss 1.439508080482483\n",
      "Batch 152 of size 11, loss 1.3126702308654785\n",
      "Batch 153 of size 4, loss 0.19091178476810455\n",
      "Batch 154 of size 19, loss 1.5321277379989624\n",
      "Batch 155 of size 8, loss 2.040414333343506\n",
      "Batch 156 of size 3, loss 4.121750354766846\n",
      "Batch 157 of size 8, loss 1.899271011352539\n",
      "Batch 158 of size 9, loss 0.7439062595367432\n",
      "Batch 159 of size 128, loss 1.7970219850540161\n",
      "Batch 160 of size 12, loss 1.140774130821228\n",
      "Batch 161 of size 20, loss 1.3305723667144775\n",
      "Batch 162 of size 31, loss 1.8599430322647095\n",
      "Batch 163 of size 128, loss 1.6949248313903809\n",
      "Batch 164 of size 1, loss 4.469083309173584\n",
      "Batch 165 of size 128, loss 1.4209810495376587\n",
      "Batch 166 of size 10, loss 0.9076326489448547\n",
      "Batch 167 of size 2, loss 0.6593396663665771\n",
      "Batch 168 of size 6, loss 1.2532042264938354\n",
      "Batch 169 of size 128, loss 1.4298861026763916\n",
      "Batch 170 of size 13, loss 1.0659703016281128\n",
      "Batch 171 of size 21, loss 1.4381581544876099\n",
      "Batch 172 of size 128, loss 1.3393540382385254\n",
      "Batch 173 of size 67, loss 1.2981128692626953\n",
      "Batch 174 of size 2, loss 1.1790529489517212\n",
      "Batch 175 of size 32, loss 1.4406613111495972\n",
      "Batch 176 of size 114, loss 1.219447135925293\n",
      "Batch 177 of size 26, loss 1.4359793663024902\n",
      "Batch 178 of size 128, loss 1.2923787832260132\n",
      "Batch 179 of size 128, loss 1.3769844770431519\n",
      "Batch 180 of size 1, loss 0.49340546131134033\n",
      "Batch 181 of size 20, loss 1.5762875080108643\n",
      "Batch 182 of size 71, loss 1.4696210622787476\n",
      "Batch 183 of size 6, loss 1.1147737503051758\n",
      "Batch 184 of size 5, loss 1.387939214706421\n",
      "Batch 185 of size 128, loss 1.2939013242721558\n",
      "Batch 186 of size 13, loss 1.6613940000534058\n",
      "Batch 187 of size 16, loss 1.127537488937378\n",
      "Batch 188 of size 9, loss 1.8547537326812744\n",
      "Batch 189 of size 23, loss 1.1659332513809204\n",
      "Batch 190 of size 18, loss 1.0617035627365112\n",
      "Batch 191 of size 14, loss 1.2876235246658325\n",
      "Batch 192 of size 22, loss 1.913449764251709\n",
      "Batch 193 of size 128, loss 1.2693431377410889\n",
      "Batch 194 of size 11, loss 1.6376452445983887\n",
      "Batch 195 of size 1, loss 1.1252349615097046\n",
      "Batch 196 of size 55, loss 1.539537787437439\n",
      "Batch 197 of size 1, loss 0.7703711986541748\n",
      "Batch 198 of size 33, loss 1.625685691833496\n",
      "Batch 199 of size 24, loss 1.678868293762207\n",
      "Batch 200 of size 128, loss 1.449908971786499\n",
      "Batch 201 of size 12, loss 1.4121299982070923\n",
      "Batch 202 of size 128, loss 1.1836868524551392\n",
      "Batch 203 of size 124, loss 1.1859163045883179\n",
      "Batch 204 of size 128, loss 1.1986632347106934\n",
      "Batch 205 of size 3, loss 1.0497982501983643\n",
      "Batch 206 of size 128, loss 1.1663404703140259\n",
      "Batch 207 of size 128, loss 1.441396713256836\n",
      "Batch 208 of size 27, loss 1.2031607627868652\n",
      "Batch 209 of size 128, loss 1.3396718502044678\n",
      "Batch 210 of size 128, loss 1.2365397214889526\n",
      "Batch 211 of size 77, loss 1.3128142356872559\n",
      "Batch 212 of size 14, loss 1.2617279291152954\n",
      "Batch 213 of size 113, loss 1.1488875150680542\n",
      "Batch 214 of size 102, loss 1.3355984687805176\n",
      "Batch 215 of size 3, loss 0.8866942524909973\n",
      "Batch 216 of size 1, loss 3.27123761177063\n",
      "Batch 217 of size 128, loss 1.401362419128418\n",
      "Batch 218 of size 128, loss 1.0902336835861206\n",
      "Batch 219 of size 128, loss 1.148048758506775\n",
      "Batch 220 of size 27, loss 1.5410959720611572\n",
      "Batch 221 of size 119, loss 1.3985886573791504\n",
      "Batch 222 of size 8, loss 1.2541465759277344\n",
      "Batch 223 of size 5, loss 2.6112220287323\n",
      "Batch 224 of size 27, loss 1.276464581489563\n",
      "Batch 225 of size 9, loss 1.285727858543396\n",
      "Batch 226 of size 6, loss 1.2373894453048706\n",
      "Batch 227 of size 78, loss 1.1879842281341553\n",
      "Batch 228 of size 100, loss 1.145801067352295\n",
      "Batch 229 of size 5, loss 1.3406383991241455\n",
      "Batch 230 of size 4, loss 1.064504623413086\n",
      "Batch 231 of size 16, loss 1.0735856294631958\n",
      "Batch 232 of size 15, loss 1.4317030906677246\n",
      "Batch 233 of size 2, loss 2.2701826095581055\n",
      "Batch 234 of size 128, loss 1.1306198835372925\n",
      "Batch 235 of size 3, loss 1.3445191383361816\n",
      "Batch 236 of size 128, loss 1.408421277999878\n",
      "Batch 237 of size 7, loss 1.1280431747436523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[70]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# See https://discuss.pytorch.org/t/gru-and-padded-sequences-tipps-and-tricks/90729.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m prediction, _ = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m loss = loss_fn(prediction, y.to(device))\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mBatch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m of size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss.item()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[69]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mModel.forward\u001B[39m\u001B[34m(self, sequence, hidden)\u001B[39m\n\u001B[32m     12\u001B[39m embedded = \u001B[38;5;28mself\u001B[39m.embedding(sequence)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# (batch_size, sequence_length, embedding_dim)\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m prediction, hidden = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.linear(hidden[\u001B[32m0\u001B[39m]), hidden\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1738\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1740\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1750\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1753\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1754\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1391\u001B[39m, in \u001B[36mGRU.forward\u001B[39m\u001B[34m(self, input, hx)\u001B[39m\n\u001B[32m   1389\u001B[39m \u001B[38;5;28mself\u001B[39m.check_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[32m   1390\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1391\u001B[39m     result = \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgru\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1394\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1395\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1396\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1397\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1398\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1399\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1400\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1401\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1402\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1403\u001B[39m     result = _VF.gru(\n\u001B[32m   1404\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   1405\u001B[39m         batch_sizes,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1412\u001B[39m         \u001B[38;5;28mself\u001B[39m.bidirectional,\n\u001B[32m   1413\u001B[39m     )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "34ec73d4eaaab2c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
