{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-30T21:58:09.925934Z",
     "start_time": "2025-06-30T21:58:09.922529Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:41:25.615296Z",
     "start_time": "2025-06-30T21:41:25.613227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:29:05.209931Z",
     "start_time": "2025-06-30T22:29:00.384592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train'].filter(lambda it: len(it['text']) <= 128).map(process).sort('length')\n",
    "validation_dataset: Dataset = dataset['validation'].filter(lambda it: len(it['text']) <= 128).map(process).sort(\n",
    "    'length')\n",
    "test_dataset: Dataset = dataset['test']\n",
    "\n",
    "corpus = concatenate_datasets([train_dataset, validation_dataset])['text']\n",
    "vocabulary = sorted(set(''.join(corpus)))\n",
    "\n",
    "# TODO: cleanup data to only have English letters\n",
    "char_to_i = {u: i for i, u in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def encode_x(char_to_i, message):\n",
    "    return torch.tensor([char_to_i[char] for char in message])\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = torch.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "train_messages = [encode_x(char_to_i, message) for message in train_dataset['text']]\n",
    "train_labels = [encode_y(label) for label in train_dataset['label']]\n",
    "validation_messages = [encode_x(char_to_i, message) for message in validation_dataset['text']]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset['label']]\n",
    "\n",
    "print('Number of train messages:', len(train_messages))\n",
    "print('Number of train labels:', len(train_labels))\n",
    "print('Number of validation messages:', len(validation_messages))\n",
    "print('Number of validation labels:', len(validation_labels))\n",
    "print(max([len(message) for message in train_messages]))\n",
    "print(len(vocabulary))\n"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "663d644f9a1146b1ab2feb6788df8c4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "320e9aa4c5254428926dfa5f62e3cac3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dc5e8f595084562b80607f2ab7a4e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4279 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99a5d4f47b0443d6a875ed702021a589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train messages: 25913\n",
      "Number of train labels: 25913\n",
      "Number of validation messages: 4279\n",
      "Number of validation labels: 4279\n",
      "128\n",
      "503\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:20:47.703779Z",
     "start_time": "2025-06-30T22:20:47.632120Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset['text'][:100]",
   "id": "5d91d7d27be6a117",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['😍',\n",
       " '💖',\n",
       " '😀',\n",
       " '👍',\n",
       " '🐂',\n",
       " 'yy',\n",
       " 'Ok',\n",
       " 'aw',\n",
       " 'ME',\n",
       " 'TV',\n",
       " '♡♡',\n",
       " 'no',\n",
       " 'Gd',\n",
       " 'ok',\n",
       " 'gut',\n",
       " 'Oke',\n",
       " 'bad',\n",
       " 'Fab',\n",
       " 'Wow',\n",
       " '🙄🙄🙄',\n",
       " 'yup',\n",
       " 'top',\n",
       " 'yep',\n",
       " 'okk',\n",
       " 'Bad',\n",
       " 'Baf',\n",
       " 'new',\n",
       " 'not',\n",
       " 'Xxx',\n",
       " 'wew',\n",
       " 'Try',\n",
       " 'Gud',\n",
       " 'Yas',\n",
       " 'E u',\n",
       " 'NYC',\n",
       " 'Hey',\n",
       " 'awe',\n",
       " 'Pay',\n",
       " 'Thx',\n",
       " 'Nope',\n",
       " 'heyy',\n",
       " 'nice',\n",
       " 'Kpai',\n",
       " 'same',\n",
       " 'Love',\n",
       " 'lame',\n",
       " 'Surp',\n",
       " 'ouch',\n",
       " 'Hell',\n",
       " 'Cute',\n",
       " 'Well',\n",
       " 'Why?',\n",
       " 'taco',\n",
       " 'Nice',\n",
       " 'best',\n",
       " 'Why?',\n",
       " 'Thop',\n",
       " 'yay!',\n",
       " 'why?',\n",
       " 'yeah',\n",
       " 'Cool',\n",
       " 'rain',\n",
       " '😶😶😶😶',\n",
       " 'lMMD',\n",
       " 'Like',\n",
       " 'with',\n",
       " 'mean',\n",
       " 'Also',\n",
       " 'Fine',\n",
       " 'like',\n",
       " 'good',\n",
       " 'awww',\n",
       " '55 o',\n",
       " 'Nise',\n",
       " 'شكرا',\n",
       " '****',\n",
       " 'Bye.',\n",
       " '*hug*',\n",
       " 'Oh no',\n",
       " 'nice!',\n",
       " 'uh oh',\n",
       " 'I did',\n",
       " 'Good.',\n",
       " 'Good!',\n",
       " 'Yep!!',\n",
       " 'gmail',\n",
       " 'Yeah.',\n",
       " 'woOt!',\n",
       " 'super',\n",
       " 'supp?',\n",
       " 'awee!',\n",
       " 'yeahh',\n",
       " 'Sucks',\n",
       " 'Bekar',\n",
       " 'Yayyy',\n",
       " 'Great',\n",
       " 'night',\n",
       " 'thanx',\n",
       " 'goood',\n",
       " 'why??']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:43:18.127243Z",
     "start_time": "2025-06-30T22:43:18.122074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "3f23bbdb9a1ce2ad",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:43:18.611019Z",
     "start_time": "2025-06-30T22:43:18.604454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        return self.linear(prediction[:, -1, :]), hidden"
   ],
   "id": "74d00197ac7e8079",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:44:07.411796Z",
     "start_time": "2025-06-30T22:43:19.326856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: try different batch sizes\n",
    "batch_size = 128\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "embedding_dim = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 1000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_messages, train_labels, batch_size)\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 1 == 0 or epoch == number_of_epoches - 1:\n",
    "        print(f'Epoch {epoch}, train loss {loss.item()}')"
   ],
   "id": "272aafaf0962bf85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 140.5164337158203\n",
      "Epoch 1, train loss 632.6210327148438\n",
      "Epoch 2, train loss 519.5572509765625\n",
      "Epoch 3, train loss 933.2319946289062\n",
      "Epoch 4, train loss 705.7045288085938\n",
      "Epoch 5, train loss 160.4541015625\n",
      "Epoch 6, train loss 307.45819091796875\n",
      "Epoch 7, train loss 271.8440856933594\n",
      "Epoch 8, train loss 154.2937469482422\n",
      "Epoch 9, train loss 238.359375\n",
      "Epoch 10, train loss 144.48666381835938\n",
      "Epoch 11, train loss 199.45755004882812\n",
      "Epoch 12, train loss 254.4681396484375\n",
      "Epoch 13, train loss 159.23524475097656\n",
      "Epoch 14, train loss 221.72213745117188\n",
      "Epoch 15, train loss 202.20468139648438\n",
      "Epoch 16, train loss 138.87362670898438\n",
      "Epoch 17, train loss 198.72479248046875\n",
      "Epoch 18, train loss 151.6739959716797\n",
      "Epoch 19, train loss 150.80471801757812\n",
      "Epoch 20, train loss 164.68215942382812\n",
      "Epoch 21, train loss 160.53231811523438\n",
      "Epoch 22, train loss 148.97027587890625\n",
      "Epoch 23, train loss 162.66848754882812\n",
      "Epoch 24, train loss 151.57351684570312\n",
      "Epoch 25, train loss 148.7698211669922\n",
      "Epoch 26, train loss 148.23184204101562\n",
      "Epoch 27, train loss 145.89410400390625\n",
      "Epoch 28, train loss 140.08082580566406\n",
      "Epoch 29, train loss 154.8087921142578\n",
      "Epoch 30, train loss 155.3376007080078\n",
      "Epoch 31, train loss 155.92701721191406\n",
      "Epoch 32, train loss 155.26052856445312\n",
      "Epoch 33, train loss 164.22381591796875\n",
      "Epoch 34, train loss 159.6320343017578\n",
      "Epoch 35, train loss 182.79861450195312\n",
      "Epoch 36, train loss 148.59158325195312\n",
      "Epoch 37, train loss 166.85977172851562\n",
      "Epoch 38, train loss 156.7137908935547\n",
      "Epoch 39, train loss 153.62408447265625\n",
      "Epoch 40, train loss 157.7382354736328\n",
      "Epoch 41, train loss 150.29823303222656\n",
      "Epoch 42, train loss 169.54766845703125\n",
      "Epoch 43, train loss 171.013427734375\n",
      "Epoch 44, train loss 149.235595703125\n",
      "Epoch 45, train loss 225.3358612060547\n",
      "Epoch 46, train loss 168.06822204589844\n",
      "Epoch 47, train loss 153.5116424560547\n",
      "Epoch 48, train loss 216.70037841796875\n",
      "Epoch 49, train loss 173.0381317138672\n",
      "Epoch 50, train loss 145.9688720703125\n",
      "Epoch 51, train loss 177.0782928466797\n",
      "Epoch 52, train loss 170.12997436523438\n",
      "Epoch 53, train loss 154.03199768066406\n",
      "Epoch 54, train loss 178.5608367919922\n",
      "Epoch 55, train loss 143.84495544433594\n",
      "Epoch 56, train loss 172.88589477539062\n",
      "Epoch 57, train loss 151.07322692871094\n",
      "Epoch 58, train loss 163.55859375\n",
      "Epoch 59, train loss 150.14044189453125\n",
      "Epoch 60, train loss 207.91651916503906\n",
      "Epoch 61, train loss 147.65167236328125\n",
      "Epoch 62, train loss 170.4163818359375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m model.zero_grad()\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m epoch % \u001B[32m1\u001B[39m == \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m epoch == number_of_epoches - \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:44:14.000113Z",
     "start_time": "2025-06-30T22:44:13.856754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "index = 1000\n",
    "message = validation_dataset['text'][index]\n",
    "label = validation_dataset['label'][index]\n",
    "print(message)\n",
    "print(label)\n",
    "encoded_message = encode_x(char_to_i, message)\n",
    "encoded_label = encode_y(label)\n",
    "print(encoded_message)\n",
    "print(encoded_label)\n",
    "x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1)\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "print(torch.nn.functional.softmax(prediction, dim=-1))"
   ],
   "id": "e2727732916a2a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedbumps suck when u gotta piss!!\n",
      "0\n",
      "tensor([53, 82, 71, 71, 70, 68, 87, 79, 82, 85,  2, 85, 87, 69, 77,  2, 89, 74,\n",
      "        71, 80,  2, 87,  2, 73, 81, 86, 86, 67,  2, 82, 75, 85, 85,  3,  3])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[0.3066, 0.3262, 0.3672]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ec28040b1a09fe2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
