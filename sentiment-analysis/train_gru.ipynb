{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:37:23.198811Z",
     "start_time": "2025-07-14T21:37:23.195841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ],
   "id": "a9dea8914236e91",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:37:24.041225Z",
     "start_time": "2025-07-14T21:37:24.036653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "cc76c8b605ea8925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:37:25.634436Z",
     "start_time": "2025-07-14T21:37:25.631426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process(example):\n",
    "    example['text'] = example['text'].strip()\n",
    "    example['length'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "\n",
    "def tokenize_messages(messages):\n",
    "    tokens = set()\n",
    "    tokenized_messages = []\n",
    "    for message in tqdm(messages):\n",
    "        doc = nlp(message)\n",
    "        tokenized_message = [token.text.lower() for token in doc]\n",
    "        tokens.update(tokenized_message)\n",
    "        tokenized_messages.append(tokenized_message)\n",
    "    return list(tokens), tokenized_messages\n",
    "\n",
    "\n",
    "def encode_x(token_to_index, tokens):\n",
    "    return torch.tensor([token_to_index[token] for token in tokens if token in token_to_index])\n",
    "\n",
    "\n",
    "# Returns a scalar with the class index (0, 1 or 2).\n",
    "def encode_y(label):\n",
    "    return torch.tensor(label)"
   ],
   "id": "163dd410cddbc3f7",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:39:29.312755Z",
     "start_time": "2025-07-14T21:37:27.828902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "max_length = float('inf')\n",
    "dataset_huggingface = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset_huggingface: Dataset = dataset_huggingface['train'].filter(lambda it: len(it['text']) <= max_length).map(process).sort('length')\n",
    "validation_dataset_huggingface: Dataset = dataset_huggingface['validation'].filter(lambda it: len(it['text']) <= max_length).map(process).sort(\n",
    "    'length')\n",
    "test_dataset_huggingface: Dataset = dataset_huggingface['test'].filter(lambda it: it['text'] is not None and len(it['text']) <= max_length).map(\n",
    "    process).sort('length')\n",
    "\n",
    "train_tokens, train_tokenized_messages = tokenize_messages(train_dataset_huggingface['text'])\n",
    "validation_tokens, validation_tokenized_messages = tokenize_messages(validation_dataset_huggingface['text'])\n",
    "test_tokens, test_tokenized_messages = tokenize_messages(test_dataset_huggingface['text'])"
   ],
   "id": "6f85cdf981c20f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed unexpectedly!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2e6ca925b3947bc8a8bca03079d71d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/31232 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f64cd1f9d7a43f895a4d21709c9067b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b73b4a37e194f06bc389e38258666a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afac07bc95b8470da43b443bac642dd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5206 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7474cc6e2b9e4c828e5b5e1691ebe088"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86e8cdc583e44b6b8ecda4e1624d5f31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31232/31232 [01:27<00:00, 355.72it/s]\n",
      "100%|██████████| 5205/5205 [00:14<00:00, 361.90it/s]\n",
      "100%|██████████| 5205/5205 [00:14<00:00, 368.50it/s]\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:39:31.156385Z",
     "start_time": "2025-07-14T21:39:31.121947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens + test_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))\n",
    "\n",
    "token_to_index = {u: i for i, u in enumerate(vocabulary)}"
   ],
   "id": "83a48a58a92b318c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t ', '\\n', '\\n\\n', ' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ', '             ', '              ', '               ', '                ', '                                           ', '                                                                                              ', '!', '\"', '\"-']\n",
      "36633\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:39:32.625055Z",
     "start_time": "2025-07-14T21:39:32.209179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_messages = [encode_x(token_to_index, tokens) for tokens in train_tokenized_messages]\n",
    "train_labels = [encode_y(label) for label in train_dataset_huggingface['label']]\n",
    "validation_messages = [encode_x(token_to_index, tokens) for tokens in validation_tokenized_messages]\n",
    "validation_labels = [encode_y(label) for label in validation_dataset_huggingface['label']]\n",
    "test_messages = [encode_x(token_to_index, tokens) for tokens in test_tokenized_messages]\n",
    "test_labels = [encode_y(label) for label in test_dataset_huggingface['label']]\n",
    "print(len(train_messages))\n",
    "print(len(train_labels))\n",
    "print(len(validation_messages))\n",
    "print(len(validation_labels))\n",
    "print(len(test_messages))\n",
    "print(len(test_labels))"
   ],
   "id": "890b7a1117c53c76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31232\n",
      "31232\n",
      "5205\n",
      "5205\n",
      "5205\n",
      "5205\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:39:34.351824Z",
     "start_time": "2025-07-14T21:39:34.289037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 20000\n",
    "print(train_dataset_huggingface['text'][index], train_tokenized_messages[index])"
   ],
   "id": "5377b20cad482166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least he`s in breakthrough performance tho. I just wanted him nominated in his own category ['at', 'least', 'he`s', 'in', 'breakthrough', 'performance', 'tho', '.', 'i', 'just', 'wanted', 'him', 'nominated', 'in', 'his', 'own', 'category']\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:49:07.058891Z",
     "start_time": "2025-07-14T21:49:07.053553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_dataset = CustomDataset(train_messages, train_labels)\n",
    "val_dataset = CustomDataset(validation_messages, validation_labels)\n",
    "test_dataset = CustomDataset(test_messages, test_labels)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO: a way to pass padding_value without closure?\n",
    "    return [\n",
    "        nn.utils.rnn.pad_sequence([x[0] for x in batch], batch_first=True, padding_value=len(vocabulary)),\n",
    "        torch.stack([x[1] for x in batch])\n",
    "    ]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = False\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=collate_fn)"
   ],
   "id": "5ecf640629283f3",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:49:07.555083Z",
     "start_time": "2025-07-14T21:49:07.551500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size, padding_value):\n",
    "    index = np.random.choice(len(xs) - batch_size + 1)\n",
    "    indices = range(index, index + batch_size)\n",
    "    return nn.utils.rnn.pad_sequence([xs[i] for i in indices], batch_first=True,\n",
    "                                     padding_value=padding_value).int(), torch.stack(\n",
    "        [ys[i] for i in indices])"
   ],
   "id": "b3d48b570cd093b7",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:49:07.965489Z",
     "start_time": "2025-07-14T21:49:07.960190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def estimate_loss(model, h0, iterations, validation_xs, validation_ys, batch_size, padding_value):\n",
    "    model.eval()\n",
    "    loses = torch.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size, padding_value)\n",
    "        validation_prediction, _ = model(validation_x_batch.to(device), h0.to(device))\n",
    "        validation_loss = F.nll_loss(validation_prediction, validation_y_batch.to(device))\n",
    "        loses[i] = validation_loss.item()\n",
    "    model.train()\n",
    "    return loses.mean()"
   ],
   "id": "ae850c6100343752",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:49:08.324331Z",
     "start_time": "2025-07-14T21:49:08.319088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "        self.log_softmax = nn.LogSoftmax(\n",
    "            dim=1)  # The negative log likelihood loss expects log-probabilities of each class.\n",
    "\n",
    "    def forward(self, sequence, hidden):\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(sequence)\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        # -> (batch_size, sequence_length, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction, hidden = self.rnn(embedded, hidden)\n",
    "        # See https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network.\n",
    "        linear_prediction = self.linear(prediction[:, -1])\n",
    "        return self.log_softmax(linear_prediction), hidden"
   ],
   "id": "60774ac30b3db966",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:49:08.746361Z",
     "start_time": "2025-07-14T21:49:08.721352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "embedding_dim, hidden_size, num_layers = 4, 128, 1\n",
    "model = Model(len(vocabulary), embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "# See https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2.\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# See https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html.\n",
    "REPORT_EVERY = 10\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "        b, _ = x.shape\n",
    "        h0 = torch.zeros(num_layers, b, hidden_size)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        y_pred, _ = model(x, h0.to(device))\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % REPORT_EVERY == REPORT_EVERY - 1:\n",
    "            last_loss = running_loss / REPORT_EVERY  # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "id": "df4ae777e1045f3c",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:50:13.192061Z",
     "start_time": "2025-07-14T21:49:09.335604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "# At most 40 should be enough.\n",
    "EPOCHS = 100\n",
    "\n",
    "best_val_loss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, val_data in enumerate(valid_dataloader):\n",
    "            val_x, val_y = val_data\n",
    "            b, _ = val_x.shape\n",
    "            h0 = torch.zeros(num_layers, b, hidden_size)\n",
    "            val_y_pred, _ = model(val_x.to(device), h0.to(device))\n",
    "            val_loss = loss_fn(val_y_pred, val_y.to(device))\n",
    "            running_val_loss += val_loss\n",
    "\n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_val_loss))\n",
    "\n",
    "    # Track the best performance, and save the model's state\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ],
   "id": "d8c847828731f295",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.0732738614082336\n",
      "  batch 20 loss: 1.0646005272865295\n",
      "  batch 30 loss: 1.0581764698028564\n",
      "  batch 40 loss: 1.0793264508247375\n",
      "  batch 50 loss: 1.0702161192893982\n",
      "  batch 60 loss: 1.065126860141754\n",
      "  batch 70 loss: 1.0789880514144898\n",
      "  batch 80 loss: 1.0699104070663452\n",
      "  batch 90 loss: 1.0726824164390565\n",
      "  batch 100 loss: 1.071716809272766\n",
      "  batch 110 loss: 1.0699143171310426\n",
      "  batch 120 loss: 1.075653088092804\n",
      "  batch 130 loss: 1.0674247026443482\n",
      "  batch 140 loss: 1.0642225861549377\n",
      "  batch 150 loss: 1.060082471370697\n",
      "  batch 160 loss: 1.0399309158325196\n",
      "  batch 170 loss: 1.0409671306610107\n",
      "  batch 180 loss: 1.0651098608970642\n",
      "  batch 190 loss: 1.0510247349739075\n",
      "  batch 200 loss: 1.0434499502182006\n",
      "  batch 210 loss: 1.0488872468471526\n",
      "  batch 220 loss: 1.0199285686016082\n",
      "  batch 230 loss: 1.0096097350120545\n",
      "  batch 240 loss: 1.007077294588089\n",
      "LOSS train 1.007077294588089 valid 1.0049313306808472\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.9794317781925201\n",
      "  batch 20 loss: 0.900380301475525\n",
      "  batch 30 loss: 0.9106609880924225\n",
      "  batch 40 loss: 0.918647974729538\n",
      "  batch 50 loss: 0.909606546163559\n",
      "  batch 60 loss: 0.9172859847545624\n",
      "  batch 70 loss: 0.904288250207901\n",
      "  batch 80 loss: 0.8763285100460052\n",
      "  batch 90 loss: 0.8970418453216553\n",
      "  batch 100 loss: 0.876633208990097\n",
      "  batch 110 loss: 0.9079578459262848\n",
      "  batch 120 loss: 0.9118975222110748\n",
      "  batch 130 loss: 0.8926387131214142\n",
      "  batch 140 loss: 0.858008474111557\n",
      "  batch 150 loss: 0.8919621646404267\n",
      "  batch 160 loss: 0.8759135425090789\n",
      "  batch 170 loss: 0.8802347242832184\n",
      "  batch 180 loss: 0.9212637662887573\n",
      "  batch 190 loss: 0.9022366523742675\n",
      "  batch 200 loss: 0.8902234673500061\n",
      "  batch 210 loss: 0.9014964520931243\n",
      "  batch 220 loss: 0.880727618932724\n",
      "  batch 230 loss: 0.8552277624607086\n",
      "  batch 240 loss: 0.8672110676765442\n",
      "LOSS train 0.8672110676765442 valid 0.8823963403701782\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.8003559529781341\n",
      "  batch 20 loss: 0.6982142269611359\n",
      "  batch 30 loss: 0.7621858358383179\n",
      "  batch 40 loss: 0.7565250217914581\n",
      "  batch 50 loss: 0.7336547434329986\n",
      "  batch 60 loss: 0.7638261139392852\n",
      "  batch 70 loss: 0.7309445381164551\n",
      "  batch 80 loss: 0.6752293050289154\n",
      "  batch 90 loss: 0.7279446542263031\n",
      "  batch 100 loss: 0.7096727371215821\n",
      "  batch 110 loss: 0.7660342514514923\n",
      "  batch 120 loss: 0.7789366066455841\n",
      "  batch 130 loss: 0.7195492923259735\n",
      "  batch 140 loss: 0.7212282836437225\n",
      "  batch 150 loss: 0.7372496962547302\n",
      "  batch 160 loss: 0.7681394398212433\n",
      "  batch 170 loss: 0.7635009229183197\n",
      "  batch 180 loss: 0.7898881375789643\n",
      "  batch 190 loss: 0.7879148423671722\n",
      "  batch 200 loss: 0.77616645693779\n",
      "  batch 210 loss: 0.784055483341217\n",
      "  batch 220 loss: 0.7854158580303192\n",
      "  batch 230 loss: 0.7821866810321808\n",
      "  batch 240 loss: 0.79177206158638\n",
      "LOSS train 0.79177206158638 valid 0.8069159388542175\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.6766579747200012\n",
      "  batch 20 loss: 0.5715284764766693\n",
      "  batch 30 loss: 0.6591548770666122\n",
      "  batch 40 loss: 0.6494365453720092\n",
      "  batch 50 loss: 0.6277205467224121\n",
      "  batch 60 loss: 0.6748761713504792\n",
      "  batch 70 loss: 0.6310615599155426\n",
      "  batch 80 loss: 0.5793650209903717\n",
      "  batch 90 loss: 0.628071403503418\n",
      "  batch 100 loss: 0.6212809205055236\n",
      "  batch 110 loss: 0.6805615425109863\n",
      "  batch 120 loss: 0.6896196722984314\n",
      "  batch 130 loss: 0.6227960884571075\n",
      "  batch 140 loss: 0.6377188622951507\n",
      "  batch 150 loss: 0.6481208503246307\n",
      "  batch 160 loss: 0.6876132547855377\n",
      "  batch 170 loss: 0.6806467354297638\n",
      "  batch 180 loss: 0.6884869039058685\n",
      "  batch 190 loss: 0.7103262484073639\n",
      "  batch 200 loss: 0.6865041196346283\n",
      "  batch 210 loss: 0.7045444548130035\n",
      "  batch 220 loss: 0.7161500036716462\n",
      "  batch 230 loss: 0.7357731223106384\n",
      "  batch 240 loss: 0.7410768985748291\n",
      "LOSS train 0.7410768985748291 valid 0.7835246920585632\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.6066497027873993\n",
      "  batch 20 loss: 0.5064105957746505\n",
      "  batch 30 loss: 0.5799621611833572\n",
      "  batch 40 loss: 0.5773698329925537\n",
      "  batch 50 loss: 0.5549289077520371\n",
      "  batch 60 loss: 0.6049318552017212\n",
      "  batch 70 loss: 0.5647367537021637\n",
      "  batch 80 loss: 0.5080203980207443\n",
      "  batch 90 loss: 0.5583203256130218\n",
      "  batch 100 loss: 0.5512032032012939\n",
      "  batch 110 loss: 0.6030116379261017\n",
      "  batch 120 loss: 0.6153119206428528\n",
      "  batch 130 loss: 0.5516891479492188\n",
      "  batch 140 loss: 0.5696855962276459\n",
      "  batch 150 loss: 0.5652926027774811\n",
      "  batch 160 loss: 0.6156398534774781\n",
      "  batch 170 loss: 0.6043525546789169\n",
      "  batch 180 loss: 0.5940445572137832\n",
      "  batch 190 loss: 0.6303831160068512\n",
      "  batch 200 loss: 0.6062328338623046\n",
      "  batch 210 loss: 0.6218560814857483\n",
      "  batch 220 loss: 0.660963374376297\n",
      "  batch 230 loss: 0.6996682703495025\n",
      "  batch 240 loss: 0.6904028952121735\n",
      "LOSS train 0.6904028952121735 valid 0.7736901640892029\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.5592193126678466\n",
      "  batch 20 loss: 0.4684165954589844\n",
      "  batch 30 loss: 0.5115686535835267\n",
      "  batch 40 loss: 0.5282547831535339\n",
      "  batch 50 loss: 0.504615867137909\n",
      "  batch 60 loss: 0.542816948890686\n",
      "  batch 70 loss: 0.5088407129049302\n",
      "  batch 80 loss: 0.4507366597652435\n",
      "  batch 90 loss: 0.5074801832437515\n",
      "  batch 100 loss: 0.4973334908485413\n",
      "  batch 110 loss: 0.5344131708145141\n",
      "  batch 120 loss: 0.5443988412618637\n",
      "  batch 130 loss: 0.481182786822319\n",
      "  batch 140 loss: 0.48966198563575747\n",
      "  batch 150 loss: 0.49444600343704226\n",
      "  batch 160 loss: 0.5601533949375153\n",
      "  batch 170 loss: 0.5467288941144943\n",
      "  batch 180 loss: 0.511603844165802\n",
      "  batch 190 loss: 0.5457037895917892\n",
      "  batch 200 loss: 0.5184109479188919\n",
      "  batch 210 loss: 0.5312984347343445\n",
      "  batch 220 loss: 0.6016545742750168\n",
      "  batch 230 loss: 0.6562204182147979\n",
      "  batch 240 loss: 0.6573168337345123\n",
      "LOSS train 0.6573168337345123 valid 0.8180990219116211\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.5251534819602967\n",
      "  batch 20 loss: 0.43583811819553375\n",
      "  batch 30 loss: 0.4606866925954819\n",
      "  batch 40 loss: 0.4820197343826294\n",
      "  batch 50 loss: 0.45488348603248596\n",
      "  batch 60 loss: 0.496067014336586\n",
      "  batch 70 loss: 0.4597281992435455\n",
      "  batch 80 loss: 0.3989743202924728\n",
      "  batch 90 loss: 0.4602992296218872\n",
      "  batch 100 loss: 0.43616133034229276\n",
      "  batch 110 loss: 0.47166775465011596\n",
      "  batch 120 loss: 0.47540070712566374\n",
      "  batch 130 loss: 0.4147996336221695\n",
      "  batch 140 loss: 0.42203004062175753\n",
      "  batch 150 loss: 0.42064149379730226\n",
      "  batch 160 loss: 0.4873924821615219\n",
      "  batch 170 loss: 0.4990139096975327\n",
      "  batch 180 loss: 0.42991308569908143\n",
      "  batch 190 loss: 0.45589098930358884\n",
      "  batch 200 loss: 0.4269654244184494\n",
      "  batch 210 loss: 0.43700036108493806\n",
      "  batch 220 loss: 0.5551483035087585\n",
      "  batch 230 loss: 0.6235343277454376\n",
      "  batch 240 loss: 0.6315583527088166\n",
      "LOSS train 0.6315583527088166 valid 0.8573380708694458\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.5010269582271576\n",
      "  batch 20 loss: 0.42413375675678255\n",
      "  batch 30 loss: 0.4228471428155899\n",
      "  batch 40 loss: 0.4398007094860077\n",
      "  batch 50 loss: 0.41573798656463623\n",
      "  batch 60 loss: 0.44853776395320893\n",
      "  batch 70 loss: 0.4224597871303558\n",
      "  batch 80 loss: 0.353760889172554\n",
      "  batch 90 loss: 0.4148677706718445\n",
      "  batch 100 loss: 0.3839049279689789\n",
      "  batch 110 loss: 0.4141374558210373\n",
      "  batch 120 loss: 0.4155228853225708\n",
      "  batch 130 loss: 0.354374086856842\n",
      "  batch 140 loss: 0.3816467344760895\n",
      "  batch 150 loss: 0.3707293182611465\n",
      "  batch 160 loss: 0.44091449975967406\n",
      "  batch 170 loss: 0.44005569219589236\n",
      "  batch 180 loss: 0.39731080532073976\n",
      "  batch 190 loss: 0.4212167978286743\n",
      "  batch 200 loss: 0.3818301439285278\n",
      "  batch 210 loss: 0.4067883133888245\n",
      "  batch 220 loss: 0.5116396844387054\n",
      "  batch 230 loss: 0.6052153766155243\n",
      "  batch 240 loss: 0.593743646144867\n",
      "LOSS train 0.593743646144867 valid 0.8524843454360962\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.47778777182102206\n",
      "  batch 20 loss: 0.38251520693302155\n",
      "  batch 30 loss: 0.4005274951457977\n",
      "  batch 40 loss: 0.3908083140850067\n",
      "  batch 50 loss: 0.36830262243747713\n",
      "  batch 60 loss: 0.3961290746927261\n",
      "  batch 70 loss: 0.37469342648983\n",
      "  batch 80 loss: 0.3026758700609207\n",
      "  batch 90 loss: 0.35846137404441836\n",
      "  batch 100 loss: 0.32856028974056245\n",
      "  batch 110 loss: 0.34519490599632263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[32m     14\u001B[39m model.train(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m avg_loss = \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m running_val_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# statistics for batch normalization.\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[93]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mtrain_one_epoch\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# Compute the loss and its gradients\u001B[39;00m\n\u001B[32m     35\u001B[39m loss = loss_fn(y_pred, y)\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Adjust learning weights\u001B[39;00m\n\u001B[32m     39\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/_tensor.py:624\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    614\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    615\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    616\u001B[39m         Tensor.backward,\n\u001B[32m    617\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    622\u001B[39m         inputs=inputs,\n\u001B[32m    623\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m624\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    625\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    823\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    826\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    827\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    829\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:50:16.111162Z",
     "start_time": "2025-07-14T21:50:16.034368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Model(len(vocabulary), embedding_dim, hidden_size, num_layers).to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(os.getcwd(), 'model_20250715_004909_4'), map_location=torch.device(device)))"
   ],
   "id": "993f5e36681145a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:50:40.319821Z",
     "start_time": "2025-07-14T21:50:17.840211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Measuring the model performance\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for index in range(len(test_dataset_huggingface)):\n",
    "    h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "    message = test_dataset_huggingface['text'][index]\n",
    "    label = test_dataset_huggingface['label'][index]\n",
    "    encoded_message = encode_x(token_to_index, test_tokenized_messages[index])\n",
    "    encoded_label = encode_y(label)\n",
    "    x_batch, y_batch = create_batch([encoded_message], [encoded_label], 1, len(vocabulary))\n",
    "    prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "    # https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network\n",
    "    _, top_i = torch.topk(prediction, k=1)\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    if labels[label] == labels[top_i[0].item()]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    if index % 100 == 0:\n",
    "        print(f'Finished test {index}, accuracy is {correct / total}')\n",
    "\n",
    "print(correct, total)\n",
    "model.train()"
   ],
   "id": "8b99638d25231845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished test 0, accuracy is 0.0\n",
      "Finished test 100, accuracy is 0.5742574257425742\n",
      "Finished test 200, accuracy is 0.6616915422885572\n",
      "Finished test 300, accuracy is 0.6677740863787376\n",
      "Finished test 400, accuracy is 0.683291770573566\n",
      "Finished test 500, accuracy is 0.6806387225548902\n",
      "Finished test 600, accuracy is 0.6905158069883528\n",
      "Finished test 700, accuracy is 0.7004279600570613\n",
      "Finished test 800, accuracy is 0.6953807740324595\n",
      "Finished test 900, accuracy is 0.6881243063263041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[96]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m h0 = torch.zeros(num_layers, \u001B[32m1\u001B[39m, hidden_size)\n\u001B[32m      7\u001B[39m message = test_dataset_huggingface[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m][index]\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m label = \u001B[43mtest_dataset_huggingface\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlabel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[index]\n\u001B[32m      9\u001B[39m encoded_message = encode_x(token_to_index, test_tokenized_messages[index])\n\u001B[32m     10\u001B[39m encoded_label = encode_y(label)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001B[39m, in \u001B[36mDataset.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   2775\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[32m   2776\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2777\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/arrow_dataset.py:2761\u001B[39m, in \u001B[36mDataset._getitem\u001B[39m\u001B[34m(self, key, **kwargs)\u001B[39m\n\u001B[32m   2759\u001B[39m format_kwargs = format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[32m   2760\u001B[39m formatter = get_formatter(format_type, features=\u001B[38;5;28mself\u001B[39m._info.features, **format_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m2761\u001B[39m pa_subtable = \u001B[43mquery_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2762\u001B[39m formatted_output = format_table(\n\u001B[32m   2763\u001B[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001B[32m   2764\u001B[39m )\n\u001B[32m   2765\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:612\u001B[39m, in \u001B[36mquery_table\u001B[39m\u001B[34m(table, key, indices)\u001B[39m\n\u001B[32m    610\u001B[39m     pa_subtable = _query_table(table, key)\n\u001B[32m    611\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m612\u001B[39m     pa_subtable = \u001B[43m_query_table_with_indices_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    613\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pa_subtable\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:72\u001B[39m, in \u001B[36m_query_table_with_indices_mapping\u001B[39m\u001B[34m(table, key, indices)\u001B[39m\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     71\u001B[39m     table = table.select([key])\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_query_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_pylist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Iterable):\n\u001B[32m     74\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _query_table(table, [indices.fast_slice(i, \u001B[32m1\u001B[39m).column(\u001B[32m0\u001B[39m)[\u001B[32m0\u001B[39m].as_py() \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m key])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/formatting/formatting.py:99\u001B[39m, in \u001B[36m_query_table\u001B[39m\u001B[34m(table, key)\u001B[39m\n\u001B[32m     97\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m table.table.slice(\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m)\n\u001B[32m     98\u001B[39m     \u001B[38;5;66;03m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfast_gather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[43m%\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnum_rows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m _raise_bad_key_type(key)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/sentiment-analysis/lib/python3.12/site-packages/datasets/table.py:124\u001B[39m, in \u001B[36mIndexedTableMixin.fast_gather\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mIndices must be non-empty\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    121\u001B[39m batch_indices = np.searchsorted(\u001B[38;5;28mself\u001B[39m._offsets, indices, side=\u001B[33m\"\u001B[39m\u001B[33mright\u001B[39m\u001B[33m\"\u001B[39m) - \u001B[32m1\u001B[39m\n\u001B[32m    122\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pa.Table.from_batches(\n\u001B[32m    123\u001B[39m     [\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mslice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_offsets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    125\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(batch_indices, indices)\n\u001B[32m    126\u001B[39m     ],\n\u001B[32m    127\u001B[39m     schema=\u001B[38;5;28mself\u001B[39m._schema,\n\u001B[32m    128\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T21:57:40.226333Z",
     "start_time": "2025-07-14T21:57:40.203783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "h0 = torch.zeros(num_layers, 1, hidden_size)\n",
    "message = \"I never hated you\"\n",
    "message = \"I do not hate you\"\n",
    "message = \"I'm sick of that\"\n",
    "message = \"My computer is great\"\n",
    "_, tokenized_messages = tokenize_messages([message])\n",
    "encoded_message = encode_x(token_to_index, tokenized_messages[0])\n",
    "x_batch, _ = create_batch([encoded_message], [torch.tensor([0, 0, 0])], 1, len(vocabulary))\n",
    "prediction, _ = model(x_batch.to(device), h0.to(device))\n",
    "print(prediction)\n",
    "_, top_i = torch.topk(prediction, k=1)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(labels[top_i[0].item()])\n",
    "model.train()"
   ],
   "id": "db55682b7562318",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 298.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8819, -1.8181, -0.8588]], device='mps:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(36633, 4)\n",
       "  (rnn): GRU(4, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33a710c1218e59da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
