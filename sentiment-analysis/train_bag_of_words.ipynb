{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-29T19:46:44.503854Z",
     "start_time": "2025-06-29T19:46:44.498987Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:47:21.194404Z",
     "start_time": "2025-06-29T19:47:21.190570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(messages):\n",
    "    tokens = set()\n",
    "    message_to_tokens = []\n",
    "    for message in messages:\n",
    "        doc = nlp(message)\n",
    "        message_tokens = {token.text.lower() for token in doc if\n",
    "                          token.pos_ in {'ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB'}}\n",
    "        tokens.update(message_tokens)\n",
    "        message_to_tokens.append(message_tokens)\n",
    "    return list(tokens), message_to_tokens"
   ],
   "id": "ab5d7274b7e5c02b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:49:34.292288Z",
     "start_time": "2025-06-29T19:49:34.289152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_x(vocabulary, tokens):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        index = vocabulary[token]\n",
    "        vector[index] += 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "def encode_y(label):\n",
    "    vector = np.zeros(3)\n",
    "    vector[label] = 1\n",
    "    return vector"
   ],
   "id": "ccdbb77242fc8586",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:48:47.745978Z",
     "start_time": "2025-06-29T19:47:25.783588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "dataset = load_dataset('Sp1786/multiclass-sentiment-analysis-dataset')\n",
    "train_dataset: Dataset = dataset['train']\n",
    "validation_dataset: Dataset = dataset['validation']\n",
    "\n",
    "train_tokens, train_message_to_tokens = tokenize(train_dataset['text'])\n",
    "validation_tokens, validation_message_to_tokens = tokenize(validation_dataset['text'])\n",
    "\n",
    "# the full list of tokens is sorted to ensure that the encoding of the messages stays the same between the Jupiter Notebook reloads, so that the saved models could be loaded and used for inference\n",
    "tokens = sorted(set(train_tokens + validation_tokens))\n",
    "print(tokens[:20])\n",
    "\n",
    "vocabulary = {token: index for index, token in enumerate(tokens)}\n",
    "print(len(vocabulary))"
   ],
   "id": "a1f577bcdf2ff5f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"-', '#', '$', '%', '&', \"'\", \"'-cholla`s\", \"'back\", \"'calendar\", \"'gummed\", \"'not\", \"'peter\", \"'s\", '(:', '(=', '*', '****', '***kix', '*shuts']\n",
      "31839\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:49:41.773738Z",
     "start_time": "2025-06-29T19:49:36.448853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_xs = np.stack([encode_x(vocabulary, tokens) for tokens in train_message_to_tokens])\n",
    "train_ys = np.stack([encode_y(label) for label in train_dataset['label']])\n",
    "validation_xs = np.stack([encode_x(vocabulary, tokens) for tokens in validation_message_to_tokens])\n",
    "validation_ys = np.stack([encode_y(label) for label in validation_dataset['label']])\n",
    "print(train_xs.shape)\n",
    "print(train_ys.shape)\n",
    "print(validation_xs.shape)\n",
    "print(validation_ys.shape)"
   ],
   "id": "70e9afa4b858fc22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31232, 31839)\n",
      "(31232, 3)\n",
      "(5205, 31839)\n",
      "(5205, 3)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:57:27.428247Z",
     "start_time": "2025-06-29T19:57:27.424640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_batch(xs, ys, batch_size):\n",
    "    random_indices = np.random.choice(len(xs), batch_size)\n",
    "    return torch.tensor(xs[random_indices], dtype=torch.float32), torch.tensor(ys[random_indices], dtype=torch.float32)"
   ],
   "id": "18c5705daf5e4493",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:57:37.141584Z",
     "start_time": "2025-06-29T19:57:37.137853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f'Device is {device}')"
   ],
   "id": "8a688ed2982f2904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:57:37.866006Z",
     "start_time": "2025-06-29T19:57:37.674614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn.Softmax is not required (see the nn.CrossEntropyLoss docs why) and can be kept only in the inference\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(vocabulary), 256),\n",
    "    nn.Linear(256, 3)\n",
    ").to(device)"
   ],
   "id": "4b8c3318eccb5afb",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:57:39.867093Z",
     "start_time": "2025-06-29T19:57:39.863215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def estimate_loss(model, iterations, validation_xs, validation_ys, batch_size):\n",
    "    model.eval()\n",
    "    loses = torch.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        validation_x_batch, validation_y_batch = create_batch(validation_xs, validation_ys, batch_size)\n",
    "        validation_prediction = model(validation_x_batch.to(device))\n",
    "        validation_loss = F.cross_entropy(validation_prediction, validation_y_batch.to(device), reduction='mean')\n",
    "        loses[i] = validation_loss.item()\n",
    "    model.train()\n",
    "    return loses.mean()"
   ],
   "id": "d3b92020a5546457",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T19:57:48.727598Z",
     "start_time": "2025-06-29T19:57:41.691210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 512\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "number_of_epoches = 100\n",
    "for epoch in range(number_of_epoches):\n",
    "    x_batch, y_batch = create_batch(train_xs, train_ys, batch_size)\n",
    "    prediction = model(x_batch.to(device))\n",
    "    loss = loss_fn(prediction, y_batch.to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == number_of_epoches - 1:\n",
    "        mean_loss = estimate_loss(model, 10, validation_xs, validation_ys, batch_size)\n",
    "        print(f'Epoch {epoch}, trail loss {loss.item()}, validation loss {mean_loss.item()}')\n",
    "    if epoch > 0 and (epoch % 10 == 0 or epoch == number_of_epoches - 1):\n",
    "        model_file_name = os.path.join(model_dir, \"model_\" + str(epoch) + \".pt\")\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        print(\"Model has been saved as\", model_file_name)\n"
   ],
   "id": "4f44062e1b42347d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, trail loss 1.1001036167144775, validation loss 1.0777775049209595\n",
      "Epoch 10, trail loss 0.8503361344337463, validation loss 0.9124234318733215\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_10.pt\n",
      "Epoch 20, trail loss 0.7822971343994141, validation loss 0.8598759770393372\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_20.pt\n",
      "Epoch 30, trail loss 0.7125304341316223, validation loss 0.8622048497200012\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_30.pt\n",
      "Epoch 40, trail loss 0.6592621207237244, validation loss 0.8717592358589172\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_40.pt\n",
      "Epoch 50, trail loss 0.5881929397583008, validation loss 0.8881387710571289\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_50.pt\n",
      "Epoch 60, trail loss 0.6059032082557678, validation loss 0.8559612035751343\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_60.pt\n",
      "Epoch 70, trail loss 0.5678271055221558, validation loss 0.8717426061630249\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_70.pt\n",
      "Epoch 80, trail loss 0.5163180828094482, validation loss 0.9362419843673706\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_80.pt\n",
      "Epoch 90, trail loss 0.5626417398452759, validation loss 0.9458433389663696\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_90.pt\n",
      "Epoch 99, trail loss 0.5126105546951294, validation loss 0.9582697749137878\n",
      "Model has been saved as /Users/yaskovdev/dev/git_home/ai-sandbox/sentiment-analysis/models/model_99.pt\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:46:47.507801Z",
     "start_time": "2025-06-28T22:46:47.424964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(\n",
    "torch.load(os.path.join(os.getcwd(), \"models\", 'model_99.pt'), map_location=torch.device(device)))"
   ],
   "id": "f4c30589c091f035",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=31839, out_features=256, bias=True)\n",
       "  (1): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T23:44:03.576994Z",
     "start_time": "2025-06-28T23:44:03.333477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "message = \"You're the worst\"\n",
    "test_tokens, _ = tokenize([message])\n",
    "x = encode_x(vocabulary, test_tokens)\n",
    "y = model(torch.tensor(x, dtype=torch.float32).to(device))\n",
    "\n",
    "distribution = torch.nn.functional.softmax(y, dim=-1)\n",
    "print(distribution)\n",
    "answer = torch.argmax(distribution)\n",
    "print(['negative', 'neutral', 'positive'][answer])\n",
    "model.train()"
   ],
   "id": "6c7160e407faace2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8409, 0.1492, 0.0099], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "negative\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T22:35:02.862627Z",
     "start_time": "2025-06-28T22:35:02.662960Z"
    }
   },
   "cell_type": "code",
   "source": "torch.onnx.export(model, (torch.tensor(x, dtype=torch.float32).to(device),), \"model.onnx\")",
   "id": "c630e4b125e46d91",
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
